---
title: "Task 1"
author: "Fabio Burri, Mischa Haenen, Robin Galeazzi, and Timon Galeazzi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, include = FALSE, warning = FALSE, echo = TRUE)
```

## Task 1

# Loading Dataset and inital exploration

For the task 1, we started by loading the dataset 'data_wage.RData'. First we download all relevant packages for this task.

```{r load packages, warning=FALSE, include=FALSE}
library(ggplot2)
library(caret)
library(ROSE)
library(readr)
library(dlookr)
library(dplyr)
library(rsample)
library(rpart)
library(rpart.plot)
library(randomForest)
library(finalfit)
library(gbm)
library(Metrics)
```

```{r load database, message=FALSE, warning=FALSE, include=FALSE}
getwd()
load("data_wage.RData")
```

First we cleaned all data from NA values. We believe that responses containing a NA are not trustworthy in terms of salary anyway and that important data for our analyses may be missing.

```{r clean dataset, message=FALSE, warning=FALSE, include=FALSE}
sum(is.na(data))
data_wage <- na.omit(data)
```

Then our idea was to get a quick overview of the dataset. Which dimensions does the dataset have? Which variable types are used?

```{r load structure, message=FALSE, warning=FALSE, include=FALSE}
dim(data_wage)
str(data_wage)
summary(data_wage)
head(data_wage)
sapply(data_wage, class)
```

We have 10'711 observations with 78 variables like gender, age, country, education, ecetera. We were then interested in getting a visual overview of the most important data and the percentages.

Our aim was then to find the dependent variable (Y), in this case 'wage'.

```{r find Y, echo=TRUE, message=FALSE, warning=FALSE}
names(data_wage)
summary(data_wage$wage)
```

We can see that the average salary is over 52,300 dollars. As the average wage is heavily distorted by high salaries - the maximum is 551,600 dollars - the median of just over 34,500 dollars is more meaningful. Because the minimum value is zero, we were interested to see how many people stated 0 as their salary (e.g. students).

```{r get students, echo=FALSE, message=FALSE, warning=FALSE}
zero_count <- sum(data_wage$wage == 0)
print(zero_count)
```

This amounted to a total of 1000 people. we then created another dataset without the people who entered 0 in the wage.

```{r create subset nozero_wage, echo=FALSE, message=FALSE, warning=FALSE}
nonzero_wage <- data_wage[data_wage$wage > 0, ]
summary(nonzero_wage$wage)
```

The average value is 57,700 dollars, the median is 43,100 dollars.Additionally, we would like to show the distribution of wages graphically.

```{r vizualising wage distribution, echo=FALSE, message=FALSE, warning=FALSE}
boxplot(data_wage$wage,
  main = "Distribution of Wages", ylab = "Wage", col = "green",
  notch = TRUE,
  outline = FALSE
)
abline(h = mean(data_wage$wage), col = "red", lwd = 2, lty = 2)

boxplot(nonzero_wage$wage,
  main = "Distribution of Wages (without wage 0)", ylab = "Wage", col = "blue",
  notch = TRUE,
  outline = FALSE
)
abline(h = mean(data_wage$wage), col = "red", lwd = 2, lty = 2)
```

It was relevant for us to work with the complete data (including all upward outliers). On the other hand, we also work with the answers that have a salary of 0. We believe that not to earn any money is also relevant for our statistics. These could be people who are looking for a job or students. This is also part of a prediction model for future salaries.

We will then analyze the categorical and numerical values separately. By looking at the data we found out, that only "wage" is a numeric variable. That's why we then focused on the most important data besides the wage: gender, age, experience, country, and education.

```{r categorial variables, echo=FALSE, message=FALSE, warning=FALSE}
# gender
gender <- table(data_wage$gender)
barplot(gender, main = "Distribution of Gender", xlab = "Gender", ylab = "Frequency", col = "blue")

# age
age <- table(data_wage$age)
barplot(age, main = "Distribution of Age", xlab = "Age", ylab = "Frequency", col = "green")

# experience
unique(data_wage$years_experience)
ordered_levels <- c("0-1", "1-2", "2-3", "3-4", "4-5", "5-11", "11-15", "15-20", "20-25", "25-30", "30 +")
data_wage$experience <- factor(data_wage$years_experience, levels = ordered_levels, ordered = TRUE)
experience_table <- table(data_wage$experience)
barplot(experience_table, main = "Distribution of Experience", xlab = "Experience", ylab = "Frequency", col = "lightblue", las = 2)

# country
country_counts <- table(data_wage$country)
country_percentages <- prop.table(country_counts) * 100
sorted_country_percentages <- sort(country_percentages, decreasing = TRUE)
top5_countries <- head(sorted_country_percentages, 5)
barplot(top5_countries, main = "Top 5 Countries by Percentage", ylab = "Percentage", col = "red", las = 2)

# education
education_counts <- table(data_wage$education)
education_percentages <- prop.table(education_counts) * 100
sorted_education_percentages <- sort(education_percentages, decreasing = TRUE)
top3_education <- head(sorted_education_percentages, 3)
barplot(top3_education, main = "Top 3 Education by Percentage", ylab = "Percentage", col = "orange")
```

After these analyses, we concentrated on the top earners. In other words, we looked for the top 5% of earners.

```{r top earners, echo=FALSE, message=FALSE, warning=FALSE}
wage_threshold <- quantile(data_wage$wage, 0.95, na.rm = TRUE)
top_earners <- data_wage[data_wage$wage >= wage_threshold, ]

dim(top_earners)
summary(top_earners$gender)
summary(top_earners$age)
summary(top_earners$experience)
summary(top_earners$country)
summary(top_earners$education)
```

We see that the top 5% of the highest earners include a total of 536 people) 491 men and 38 women. The analysis of the top 5% shows that people between the ages of 30 and 34 earn the most. In terms of experience, it is people with between 5 and 11 years of work experience who earn the most. 380 of the people come from the USA, which is no surprise in this case, as most of the respondents come from the USA. However, it is interesting to note that 255 of the 536 people have a master's degree, while only 160 have a doctorate.

Then our goal was to scale numeric data to ensure that features have a similar range of values.

```{r scale wage, echo=FALSE, message=FALSE, warning=FALSE}
numeric_columns <- sapply(data_wage, is.numeric)
data[numeric_columns] <- scale(data_wage[numeric_columns])

min_max_scale <- function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
data_wage[numeric_columns] <- apply(data_wage[numeric_columns], 2, min_max_scale)

summary(data_wage)
```

After that our goal was to convert categorical variables into a format suitable for modeling.

```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# One-hot encode categorical variables
data_wage$years_experience <- NULL

View(data_wage)

# Apply dummy variable encoding
dummies <- dummyVars(" ~ .", data = data_wage)
data_transformed <- as.data.frame(predict(dummies, newdata = data_wage))

# Add the target variable back to the transformed data
data_transformed$wage <- data_wage$wage
```

######################################## Notizen

The second step, removing imbalances between classes, is somehow more difficult. It is particularly crucial when dealing with classification tasks where the distribution of the classes in the target variable is significantly skewed. This situation can lead to models that are biased towards the majority class, potentially neglecting the minority class, which can be problematic, especially if the minority class is of great interest (e.g., in fraud detection, disease diagnosis).

When to Use Class Imbalance Techniques:

Classification Tasks: When your dependent variable is categorical and youâ€™re predicting class membership (e.g., yes/no, pass/fail). Skewed Class Distribution: When one or more classes dominate the dataset, which is often visible through a simple table() of the target variable. Critical Outcomes: When incorrect classification of the minority class has significant consequences, such as in medical diagnoses or credit approval.

######################################## Notizen

Now it is time to decide which variables (characteristics) to include in the model, based on their potential relationship to the target variable "wage".


```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Compute correlation matrix for numeric variables
numeric_data <- data_wage[, sapply(data_wage, is.numeric)]
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_wage <- cor_matrix[, "wage"]
cor_wage <- sort(cor_wage, decreasing = TRUE)
cor_wage
```

```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Split data into training and testing sets
set.seed(123)
train_index <- createDataPartition(data_transformed$wage, p = 0.8, list = FALSE)
train_data <- data_transformed[train_index, ]
test_data <- data_transformed[-train_index, ]

# Replace spaces with periods in column names
colnames(train_data) <- make.names(colnames(train_data))
colnames(test_data) <- make.names(colnames(test_data))

# Create a formula for the model excluding the target variable
predictors <- setdiff(names(train_data), "wage")
formula <- as.formula(paste("wage ~", paste(predictors, collapse = " + ")))

# Verify the updated formula
print(formula)

hist(train_data$wage)
hist(test_data$wage)
```

```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Train a model explaining wage (i.e. Y), specify independent variables (i.e. Xs)
linear_model <- lm(formula, data = train_data)

# Print results of the model
summary(linear_model)

# Make predictions on the test data to evaluate your model's performance
predictions_linear <- predict(linear_model, test_data)

# Compute the prediction error, RMSE
cat("RMSE (Linear Regression):", RMSE(predictions_linear, test_data$wage), "\n")
```


```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Train a model explaining wage (i.e. Y), specify independent variables (i.e. Xs)
randomForest_model <- randomForest(formula, data = train_data, ntree = 50, mtry = 3, importance = TRUE)

# Plot variable importance
varImpPlot(randomForest_model)

# Make predictions on the test data to evaluate your model's performance
predictions_rf <- predict(randomForest_model, test_data)

# Compute the prediction error, RMSE
cat("RMSE (Random Forest Regression):", RMSE(predictions_rf, test_data$wage), "\n")
```


```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Train a model explaining wage (i.e. Y), specify independent variables (i.e. Xs)
gbm_model <- train(formula, data = train_data, method = "gbm", verbose = FALSE, trControl = trainControl(method = "cv", number = 5), tuneGrid = expand.grid(n.trees = 100, interaction.depth = 3, shrinkage = 0.1, n.minobsinnode = 10))

# Print results of the model
print(gbm_model)

# Make predictions on the test data to evaluate your model's performance
predictions_gbm <- predict(gbm_model, test_data)

# Compute the prediction error, RMSE
cat("RMSE (Gradient Boosting):", RMSE(predictions_gbm, test_data$wage), "\n")
```


```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Compute the prediction error, RMSE
rmse_linear <- RMSE(predictions_linear, test_data$wage)
rmse_rf <- RMSE(predictions_rf, test_data$wage)
rmse_gbm <- RMSE(predictions_gbm, test_data$wage)

# Print RMSE values
cat("RMSE (Linear Regression):", rmse_linear, "\n")
cat("RMSE (Random Forest):", rmse_rf, "\n")
cat("RMSE (Gradient Boosting):", rmse_gbm, "\n")
```

```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Compare models and select the best one
rmse_values <- c(linear = rmse_linear, rf = rmse_rf, gbm = rmse_gbm)
best_model_name <- names(which.min(rmse_values))
best_model <- switch(best_model_name,
  linear = linear_model,
  rf = randomForest_model,
  gbm = gbm_model
)
cat("Best model is:", best_model_name, "with RMSE:", min(rmse_values), "\n")
```

```{r one-hot encoding, echo=FALSE, message=FALSE, warning=FALSE}
# Variable importance for the best model
if (best_model_name == "linear") {
  importance <- summary(best_model)$coefficients
} else if (best_model_name == "rf") {
  importance <- importance(best_model)
  varImpPlot(best_model)
} else if (best_model_name == "gbm") {
  importance <- varImp(best_model)
  plot(importance, main = "Variable Importance - Gradient Boosting")
}

print(importance)
```

