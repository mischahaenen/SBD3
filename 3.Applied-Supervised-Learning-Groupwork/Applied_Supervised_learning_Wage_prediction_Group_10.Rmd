---
title: "Task 1: Wage Prediction Model: Data Exploration, Model Selection, and Prediction"
author: "Fabio Burri, Mischa Haenen, Robin Galeazzi, and Timon Galeazzi"
date: "`r Sys.Date()`"
output: pdf_document
---

## Data Exploration and Preparation: Analyze the features in your dataset to select relevant predictors for the wage.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(dplyr)
library(caret)
library(rsample)
library(rpart)
library(rpart.plot)
library(randomForest)
library(readr)
library(gbm)
library(Metrics)
library(ggplot2)
library(pROC)
library(ROCR)
library(dlookr)
library(finalfit)
library(h2o)
library(data.table)
library(writexl)
library(xgboost)
```

```{r}
# Load the dataset
# getwd()
# setwd("./3.Applied-Supervised-Learning-Groupwork")
load("./data_wage.RData")

# Handle missing values by removing rows with NAs
data_wage <- na.omit(data)
```

Then our idea was to get a quick overview of the dataset. Which dimensions does the dataset have? Which variable types are used?

```{r}
# Display dataset dimensions and basic structure
dim(data_wage)
summary(data_wage)
head(data_wage, 1)
hist(data_wage$wage, breaks = 30, main = "Distribution of Wages", xlab = "Wage")
```

We have 10'711 observations with 78 variables like gender, age, country, education, ecetera. We were then interested in getting a visual overview of the most important data and the percentages.

Our aim was then to find the dependent variable (Y), in this case 'wage'.

```{r}
# Analyze the dependent variable 'wage'
summary(data_wage$wage)
```

We can see that the average salary is over 52,300 dollars. As the average wage is heavily distorted by high salaries - the maximum is 551,600 dollars - the median of just over 34,500 dollars is more meaningful. Because the minimum value is zero, we were interested to see how many people stated 0 as their salary (e.g. students).

```{r create subset nozero_wage, echo=FALSE, message=FALSE, warning=FALSE}
# Count number of observations with zero wage
zero_count <- sum(data_wage$wage == 0)
cat("Number of observations with zero wage:", zero_count, "\n")

# Create a dataset without zero wages
nonzero_wage <- data_wage %>% filter(wage > 0)
```

This amounted to a total of 1000 people. we then created another dataset without the people who entered 0 in the wage. The average value is 57,700 dollars, the median is 43,100 dollars.Additionally, we would like to show the distribution of wages graphically.

```{r vizualising wage distribution, echo=FALSE, message=FALSE, warning=FALSE}
# Boxplot and histogram for wage distribution
boxplot(data_wage$wage,
  main = "Distribution of Wages", ylab = "Wage", col = "green",
  notch = TRUE,
  outline = FALSE
)
abline(h = mean(data_wage$wage), col = "red", lwd = 2, lty = 2)

data_wage %>%
  ggplot(aes(x = wage)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Wages", x = "Wage", y = "Frequency")

boxplot(nonzero_wage$wage,
  main = "Distribution of Wages (without wage 0)",
  ylab = "Wage", col = "blue",
  notch = TRUE,
  outline = FALSE
)
abline(h = mean(data_wage$wage), col = "red", lwd = 2, lty = 2)

nonzero_wage %>%
  ggplot(aes(x = wage)) +
  geom_histogram(bins = 30, fill = "lightgreen", color = "black") +
  labs(
    title = "Distribution of Wages (without zero wages)",
    x = "Wage", y = "Frequency"
  )
```

It was relevant for us to work with the complete data (including all upward outliers). On the other hand, we also work with the answers that have a salary of 0. We believe that not to earn any money is also relevant for our statistics. These could be people who are looking for a job or students. This is also part of a prediction model for future salaries.

We will then analyze the categorical and numerical values separately. By looking at the data we found out, that only "wage" is a numeric variable. That's why we then focused on the most important data besides the wage: gender, age, experience, country, and education.

```{r categorial variables, echo=FALSE, message=FALSE, warning=FALSE}
# Convert experience into an ordered factor and remove original column
data_wage <- data_wage %>%
  mutate(
    experience = factor(
      years_experience,
      levels = c(
        "0-1", "1-2", "2-3", "3-4", "4-5", "5-11",
        "11-15", "15-20", "20-25", "25-30", "30 +"
      ),
      ordered = TRUE
    )
  ) %>%
  select(-years_experience)

# Visualize distribution of categorical variables
categorical_vars <- c("gender", "age", "country", "education")
for (var in categorical_vars) {
  data_wage %>%
    ggplot(aes(x = factor(!!sym(var)))) +
    geom_bar(fill = "steelblue") +
    labs(title = paste("Distribution of", var), x = var, y = "Frequency") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```
After these analyses, we concentrated on the top earners. In other words, we looked for the top 5% of earners.
```{r top earners, echo=FALSE, message=FALSE, warning=FALSE}
# Analyze top 5% earners
top_earners <- data_wage %>%
  filter(wage >= quantile(wage, 0.95, na.rm = TRUE))

# Summarize top earners' characteristics
dim(top_earners)
summary(top_earners$gender)
summary(top_earners$age)
summary(top_earners$experience)
summary(top_earners$country)
summary(top_earners$education)
```
We see that the top 5% of the highest earners include a total of 536 people) 491 men and 38 women. The analysis of the top 5% shows that people between the ages of 30 and 34 earn the most. In terms of experience, it is people with between 5 and 11 years of work experience who earn the most. 380 of the people come from the USA, which is no surprise in this case, as most of the respondents come from the USA. However, it is interesting to note that 255 of the 536 people have a master's degree, while only 160 have a doctorate.

Then our goal was to scale numeric data to ensure that features have a similar range of values. After that our goal was to convert categorical variables into a format suitable for modeling.

```{r }
# One-hot encode categorical variables and scale numeric variables
data_encoded <- data_wage %>%
  mutate(across(where(is.character), factor)) %>%
  mutate(across(where(is.numeric), scale)) %>%
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.x)) - 1))
# Add the target variable back to the transformed data
dummies <- dummyVars(" ~ .", data = data_encoded)
data_transformed <- as.data.frame(predict(dummies, newdata = data_encoded))
# Add the target variable back
data_transformed$wage <- data_wage$wage
```

Now it is time to decide which variables (characteristics) to include in the model, based on their potential relationship to the target variable "wage".

## Model Selection and Training:

In this chapter we have trained different models (linear regression, regression tree, random forest) in order to find the best performing one. The models were trained with the data set data_transformed.

```{r}
# Split data into training and testing sets
set.seed(123)
split <- initial_split(data_transformed, prop = 0.8)
train_data <- training(split)
test_data <- testing(split)
# Helper function for model training and evaluation
train_and_evaluate_model <- function(model_type, data) {
  set.seed(123)
  # Define cross-validation method: Cross Validation with 5 folds means that the data is split into 5 parts, and the model is trained on 4 parts and tested on the 5th part. This process is repeated 5 times.
  train_control <- trainControl(method = "cv", number = 5)

  if (model_type == "xgbTree") {
    data_no_wage <- data[, !names(data) %in% "wage"]
    model <- train(
      x = data_no_wage,
      y = data$wage,
      method = model_type,
      trControl = train_control,
      tuneLength = 3
    )
  } else {
    model <- train(
      wage ~ .,
      data = data,
      method = model_type,
      trControl = train_control
    )
  }
  return(model)
}
# Train models
lm_model <- train_and_evaluate_model("lm", train_data)
# We have to use the randomForest package for the random forest model and not the train_and_evaluate_model function otherwise  varImpPlot will throw an type error.
rf_model <- randomForest(
  wage ~ .,
  data = train_data, ntree = 50, mtry = 3, importance = TRUE, cp = 0
)
gbm_model <- train_and_evaluate_model("gbm", train_data)
xgboost_model <- train_and_evaluate_model("xgbTree", train_data)

# Hyperparameter tuning using Grid Search and Random Search
# This is an example for Random Forest and GBM models, we could fine tune the other models as well but for the sake of time we will not do this.
set.seed(123)
# Grid search for Random Forest
rf_grid <- expand.grid(mtry = c(2, 3, 4))
rf_control <- trainControl(method = "cv", number = 5, search = "grid")
rf_tuned <- train(
  wage ~ .,
  data = train_data, method = "rf",
  trControl = rf_control, tuneGrid = rf_grid
)

# Random search for GBM
set.seed(123)
gbm_grid <- expand.grid(
  n.trees = c(100, 150, 200),
  interaction.depth = c(1, 3, 5),
  shrinkage = c(0.01, 0.1),
  n.minobsinnode = 10
)
gbm_control <- trainControl(method = "cv", number = 5, search = "random")
gbm_tuned <- train(
  wage ~ .,
  data = train_data, method = "gbm",
  trControl = gbm_control, tuneGrid = gbm_grid, verbose = FALSE
)
```

## Model Evaluation

Following the training of the models we evaluate here which modes is the best performing. We will asses this using the metric RMSE.

```{r}
# Define your model list (including tuned models if applicable)
model_list <- list(
  "Linear Regression" = lm_model,
  "Random Forest" = rf_model,
  "Gradient Boosting" = gbm_model,
  "XGBoost" = xgboost_model,
  "Random Forest Tuned" = rf_tuned,
  "Gradient Boosting Tuned" = gbm_tuned
)

# Create empty lists to store results
predictions_list <- list()
rmse_values <- c()
mae_values <- c()
r2_values <- c()

# Iterate through the models
for (model_name in names(model_list)) {
  model <- model_list[[model_name]]
  # Make predictions
  predictions <- predict(model, newdata = test_data)
  predictions_list[[model_name]] <- predictions # Store predictions
  # Calculate and store metrics
  rmse_values[model_name] <- RMSE(predictions, test_data$wage)
  mae_values[model_name] <- MAE(predictions, test_data$wage)
  r2_values[model_name] <- R2(predictions, test_data$wage)
  # Print metrics for each model
  cat("Metrics for", model_name, "model:\n")
  cat("RMSE (", model_name, "):", rmse_values[model_name], "\n")
  cat("MAE (", model_name, "):", mae_values[model_name], "\n")
  cat("R-squared (", model_name, "):", r2_values[model_name], "\n")
  cat("\n")
}

# Create data for visualization
comparison <- data.frame(Actual = test_data$wage)
for (model_name in names(predictions_list)) {
  comparison[[model_name]] <- predictions_list[[model_name]]
}

# Reshape data for ggplot
comparison_long <- tidyr::pivot_longer(comparison,
  -Actual,
  names_to = "Model", values_to = "Predicted"
)

# Visual comparison
ggplot(comparison_long, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point() +
  labs(
    title = "Predicted vs. Actual Wages",
    x = "Actual Wage", y = "Predicted Wage"
  ) +
  theme_minimal() +
  facet_wrap(~Model)
```

The Gradient Boosting and XGBoost models show the best performance, with Gradient Boosting slightly edging out XGBoost in terms of RMSE and MAE.

Here's a breakdown of the model performance:

- RMSE (Root Mean Squared Error): Gradient Boosting has a slightly higher RMSE (43573.9) compared to XGBoost (43005.61), suggesting a marginally smaller average prediction error.
- MAE (Mean Absolute Error): Similarly, Gradient Boosting has a slightly higher MAE (24697.86) than XGBoost (23676.49), indicating a slightly smaller average absolute deviation from the actual wages.
- R-squared: Both models have very high R-squared values (0.503 for Gradient Boosting and 0.519 for XGBoost), indicating they explain a substantial portion of the variance in wages.

Interpretation of the Plot:
The plot visually supports these findings. The Gradient Boosting (red) and XGBoost (purple) predictions are the most tightly clustered around the diagonal line representing a good prediction. This shows their better accuracy compared to the other models. The other model predictions are more scattered.

Important Considerations:
- Marginal Difference: The difference in performance between Gradient Boosting and XGBoost is very small. It does not care which model we chose. We will predict the wages for the team members with both models.
- Overfitting Risk: There might be a slight risk of overfitting for Gradient Boosting and XGBoost due to their high R-squared values.
- Random Forest: The Random Forst Modell is not trained with the model function and therefore a fair comparison is not possible. However, the Random Forest model is also a good model for predicting wages.

## Model Explanation

Here we aim to see which variables have the highest correlation with the variable wage. We will use the variable importance plot to see which variables are the most important for the model.
```{r}
# Variable importance for Random Forest model
varImpPlot(rf_model)
# Summary for all models
for (model_name in names(model_list)) {
  model <- model_list[[model_name]]
  var_imp <- varImp(model)
  cat("Variable Importance for", model_name, "Model:\n")
  print(var_imp)
  cat("\n")
  summary(model)
  cat("\n")
}
```

### Insights from Model Comparison (General):
Several key factors show a strong explainability of wages across all models, indicating their fundamental importance in determining wages:

- Country: Geographic country plays a major role, likely reflecting variations in cost of living, industry concentration, and demand for machine learning talent.
- Age and Experience: Age and years of experience are consistently strong predictors, suggesting that wages generally increase with professional maturity and expertise.
- Job Role: The specific job role within the field of machine learning significantly impacts salary, as different roles require varying levels of specialization and responsibility.
- Coding Experience (for Data Analysis): The length of time spent coding for data analysis emerges as a crucial factor, highlighting the value placed on practical programming skills.
- Machine Learning Usage at Work (ML_atwork): Whether or not someone utilizes machine learning in their work is a consistent predictor, underscoring the premium placed on applied ML skills.
- Cloud Experience (AWS): Experience with Amazon Web Services (AWS) stands out as a significant factor, reflecting the widespread adoption of AWS in the industry and the demand for cloud expertise.
- Domain-Specific Factors: Various domain-specific variables, such as those relating to specific machine learning activities or framework usage, also demonstrate considerable influence.

### Model-Specific Insights
#### Linear Regression:
The linear regression model heavily emphasizes country, age, and experience, aligning with the expectation of linear relationships between these factors and salary. AWS experience is notably prominent, suggesting a potential salary premium for professionals with this skill.
#### Random Forest:
Compared to linear regression, the random forest model distributes importance more evenly across various predictors. country, age, job role, and experience remain significant factors. Notably, specific programming languages and tools gain importance, indicating that tool preferences might influence salary in complex, non-linear ways.
#### Gradient Boosting:
Similar to the random forest, but with a stronger focus on country, age, and job role. Time spent coding for data analysis emerges as a top predictor, reinforcing the value of practical coding skills in this domain.
#### XGBoost:
The XGBoost model closely aligns with the Gradient Boosting model in terms of variable importance, with country, age, coding experience, and job role as the top predictors. This allignment between the two top-performing models provides strong evidence for the relevance of these factors in determining salary.
#### Tuned Models (Random Forest and Gradient Boosting):
The tuned models generally exhibit similar variable importance patterns as their untuned counterparts, with minor shifts in rankings. This suggests that the overall relationships between predictors and salary remain relatively stable, even after hyperparameter tuning.

## Wage Prediction:

We have decided to use gender, age, country, education and experience.

```{r}
# Load and prepare the team data
set.seed(123)
team_data <- read_csv("./team_data.csv")

# Add experience column with consistent levels
team_data$experience <- factor(
  team_data$years_experience,
  levels = c(
    "0-1", "1-2", "2-3", "3-4", "4-5", "5-11",
    "11-15", "15-20", "20-25", "25-30", "30 +"
  ),
  ordered = TRUE
)
# Remove years_experience column and first column
team_data <- team_data[, -1] %>% select(-years_experience)
# Ensure factor levels in team_data match those in data_wage
for (col in names(team_data)) {
  if (is.character(team_data[[col]])) {
    levels_in_wage_data <- levels(data_wage[[col]])
    team_data[[col]] <- factor(team_data[[col]], levels = levels_in_wage_data)
  }
}
# One-hot encode categorical variables and scale numeric variables
team_data_encoded <- team_data %>%
  mutate(across(
    where(is.character),
    ~ factor(.x, levels = levels(data_wage[[cur_column()]]))
  )) %>%
  mutate(across(where(is.numeric), ~ scale(.x))) %>%
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.x)) - 1))
# Apply dummy variable encoding
team_data_transformed <- as.data.frame(
  predict(dummies, newdata = team_data_encoded)
)
# replace NAs with 0
team_data_transformed[is.na(team_data_transformed)] <- 0
# Use XGBoost model for prediction
pred_xgboost <- predict(xgboost_model, newdata = team_data_transformed)
print(pred_xgboost)
# Use Gradient Boosting model for prediction
pred_gbm <- predict(gbm_model, newdata = team_data_transformed)
print(pred_gbm)
```
Until the end we could not factor the data_wage properties to our custom team data set which is why we could not predict the wages for the team members.
### Interpreting the Predictions:

- XGBoost: Predicts wages of 57900, 28384, 40068, and 46464 for the four team members, respectively.
- Gradient Boosting: Predicts wages of 28587, 16727, 20804, and 25892 for the four team members. Here when the NA values would not be replaced with 0, the predictions would not work.
### Noticeable Differences:

The XGBoost model predicts significantly higher wages for the first and fourth team members compared to the Gradient Boosting model. We think this is due to the different ways the models handle the relationships between predictors and wages.
### Impact of Replacing NAs with 0:

Replacing missing values (NAs) with zeros will propably distort the predictions:

- Meaning Change: Zeros has a specific meaning in data (e.g., absence of a characteristic). Replacing NAs with zeros has changed the interpretation of the data.
- Skewed Relationships: Zeros skew the relationships between variables. If a model learned a relationship based on actual zeros, it make incorrect assumptions.
- Loss of Information: Replacing NAs with zeros discards information.

## H2O AutoML analysis

```{r}
# H2O AutoML Setup
h2o.init()

# Convert the dataset to H2O object
h2o_data <- as.h2o(data_transformed)

# Split dataset into training and validation sets
splits <- h2o.splitFrame(h2o_data, ratios = c(0.8), seed = 12)
train <- splits[[1]]
valid <- splits[[2]]

# Set the dependent variable
dep_var <- "wage"

# Run H2O AutoML
automl <- h2o.automl(
  x = setdiff(colnames(h2o_data), dep_var),
  y = dep_var,
  training_frame = train,
  max_runtime_secs = 600,
  seed = 12
)

# View leaderboard of models generated by AutoML
lb <- automl@leaderboard
print(lb, n = nrow(lb))

# Export leaderboard to Excel
lb_table <- as.data.table(lb)
write_xlsx(lb_table, "./trained_ML_models.xlsx")

# Find the best performing model per a certain criterion and explore it
best_model <- h2o.get_best_model(automl, criterion = "rmse")
best_model

# Predictions and performance on the validation set
pred_best_model <- h2o.predict(best_model, valid)
perf_best_model <- h2o.performance(best_model, valid)

# Summarize the performance
rmse <- h2o.rmse(perf_best_model)
mae <- h2o.mae(perf_best_model)
r2 <- h2o.r2(perf_best_model)

cat("RMSE (Stacked Ensemble):", rmse, "\n")
cat("MAE (Stacked Ensemble):", mae, "\n")
cat("R-squared (Stacked Ensemble):", r2, "\n")

# Variable Importance for Stacked Ensemble Model
summary(best_model)

# Use the best-performing model from H2O AutoML for predictions
pred_best_model <- as.data.frame(h2o.predict(best_model, as.h2o(team_data_transformed)))
predicted_wages <- pred_best_model$predict
print(predicted_wages)

# Conclusion
cat(
  "The best-performing model was", best_model@model_id,
  "with an RMSE of", rmse, ", MAE of", mae, ", and R-squared of", r2, ".\n"
)
```
We see that h2o did not lead to a better model.
### Model Details:
Type: Stacked ensemble model (H2ORegressionModel: stackedensemble), meaning it combines predictions from multiple base models to make a final prediction.
Stacking Strategy: The ensemble uses cross-validation to train and combine the base models, same as we used previously
Base Models: It uses 11 out of a possible 43 base models. These are all Gradient Boosting Machine (GBM) models.
Metalearner: A Generalized Linear Model (GLM) is used as the metalearner, which learns how to best combine the predictions of the base models.

Still the h2o model did not lead to a better model. We will stick to the XGBoost model for our predictions.
