# ----------------------------------------------------------------------------- #
# ----------------------------------------------------------------------------- #
# ------------------------------ AUTO ML IN R --------------------------------- #
# ----------------------------------------------------------------------------- #
# ----------------------------------------------------------------------------- #


# R interface for 'H2O', the scalable open source machine learning platform that
# offers parallelized implementations of many supervised and unsupervised machine
# learning algorithms such as:
# - Generalized Linear Models (GLM),
# - Gradient Boosting Machines (including XGBoost),
# - Random Forests,
# - Deep Neural Networks (Deep Learning),
# - Stacked Ensembles,
# - Naive Bayes,
# - Generalized Additive Models (GAM),
# - ....

# as well as a fully automatic machine learning algorithm (H2O AutoML).


# Within this script, we will learn how to initiate a cluster and run H20's AutoML
# which can be used for automating the machine learning workflow.


# Setting up H20
# Java is a prerequisite for H2O, even if using it from the R or Python packages.
# You must have JAVA installed on your PC.
# *For Mac: https://www.java.com/en/download/apple.jsp
# *For Windows: https://www.java.com/download/ie_manual.jsp


# For further details, please check: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html


# Installing & initializing H20. In case you have past installations, you should run
# lines 37-44. If this is the first time installing, you can only run lines 43-44.
if ("package:h2o" %in% search()) {
  detach("package:h2o", unload = TRUE)
}
if ("h2o" %in% rownames(installed.packages())) {
  remove.packages("h2o")
}
pkgs <- c("RCurl", "jsonlite")
for (pkg in pkgs) {
  if (!(pkg %in% rownames(installed.packages()))) {
    install.packages(pkg)
  }
}
install.packages("h2o", type = "source", repos = (c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)


# Call other necessary libraries
library(data.table)
library(writexl)
library(readr)
install.packages("writexl")

# Initialize H2O cluster
h2o.init()


# Load dataset
setwd("~/Library/CloudStorage/OneDrive-SharedLibraries-BernerFachhochschule/bfh-bfhinternal - General/SBD3/Week 5")
# df <- h2o.importFile("Employee-Attrition_cleaned.csv")
# df <- as.h2o(df)


df <- read_csv("./data/Employee-Attrition_cleaned.csv")
str(df) # check the structure of the dataset
summary(df) # check the summary statistics


# Identify the categorical variables in the dataset
cat_cols <- c()
for (col in names(df)) {
  if (class(df[[col]]) == "factor" | typeof(df[[col]]) == "character") {
    cat_cols <- c(cat_cols, col)
  }
}

# Convert the categorical variables to factors
df[cat_cols] <- lapply(df[cat_cols], factor)
str(df)

# Set the dataframe as h2o environment
df <- as.h2o(df)

# Split dataset into training and validation sets
set.seed(12)
splits <- h2o.splitFrame(df, ratios = c(0.8))
train <- splits[[1]]
valid <- splits[[2]]


# Set the dependent variable
dep_var <- "Attrition"

# Run AutoML

# The current version of AutoML trains and cross-validates the following algorithms:
# * three pre-specified XGBoost GBM (Gradient Boosting Machine) models,
# * a fixed grid of GLMs,
# * a default Random Forest (DRF),
# * five pre-specified H2O GBMs,
# * a near-default Deep Neural Net,
# * an Extremely Randomized Forest (XRT),
# * a random grid of XGBoost GBMs,
# * a random grid of H2O GBMs, and
# * a random grid of Deep Neural Nets.

# In some cases, there will not be enough time to complete all the algorithms,
# so some may be missing from the leaderboard. In other cases, the grids will
# stop early, and if there’s time left, the top two random grids will be
# restarted to train more models. AutoML trains multiple Stacked Ensemble models
# throughout the process (more info about the ensembles below).

automl <- h2o.automl(
  x = setdiff(colnames(df), dep_var), # independent variables
  y = dep_var, # dependent variable
  training_frame = train,
  max_runtime_secs = 600, # maximum time in seconds for AutoML to run
  seed = 12 # set seed for reproducibility
)


# View leaderboard of models generated by AutoML
lb <- automl@leaderboard
print(lb, n = nrow(lb))


# We can also export the table and observe the result more in-depth.
lb_table <- as.data.table(lb)
write_xlsx(lb_table, "./data/trained_ML_models.xlsx") # The excel file will be located in your set working directory.


# Find the best performing model per a certain criteria and explore it.
best_auc <- h2o.get_best_model(automl, criterion = "auc") # Best model per the AUC indicator.
best_auc # Let's explore the best performing model


# Find the best performing models across different criteria (logloss, area under the precision-recall curve (aucpr), etc.)
# Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value
# (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value,
# the higher is the log-loss value.
best_logloss <- h2o.get_best_model(automl, criterion = "logloss")
best_aucpr <- h2o.get_best_model(automl, criterion = "aucpr")


# Get training timing info
trainingInfo <- automl@training_info
trainingInfo


# Predictions and performance on our validation sample.
# Obtain the predictions for our validation subset
pred_best_auc <- h2o.predict(best_auc, valid)
predictions <- as.data.table(pred_best_auc)
perf_best_auc <- h2o.performance(best_auc, valid)

# Summarize the performance.
# We build a function that can extract the individual performance indicators like
# auc, loglog and aucpr for the best performing model.
performance_single <- function(perf_object) {
  capture.output(cat(
    "logloss", h2o.logloss(perf_object),
    "auc", h2o.auc(perf_object),
    "aucpr", h2o.aucpr(perf_object)
  ))
}

# Check the results for the best performing model (by auc value)
single_best_auc <- performance_single(perf_best_auc)
single_best_auc


# Obtain the confusion matrix
conf_matrix_best_auc <- h2o.confusionMatrix(perf_best_auc)
conf_matrix_best_auc


# Plot the ROC and the Precision Recall curve
plot(perf_best_auc, type = "roc")
plot(perf_best_auc, type = "pr")


# ----------------------------------------------------------------------------- #
# ----------------------------------------------------------------------------- #
# ----------------------- Explainability in H20 ----------- ------------------- #
# ----------------------------------------------------------------------------- #
# ----------------------------------------------------------------------------- #

# H20 framework explained (for further details check the documentation:
# https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html)
# H2O Explainability Interface is a convenient wrapper to a number of explainabilty
# methods and visualizations in H2O. The main functions:
#  * h2o.explain() (global explanation)
#  * h2o.explain_row() (local explanation)
# work for individual H2O models, as well a list of models or an H2O AutoML object.

# The h2o.explain() function generates a list of explanations – individual units
# of explanation such as a Partial Dependence plot or a Variable Importance plot.
# Most of the explanations are visual – these plots can also be created by individual
# utility functions outside the h2o.explain() function. The visualization engine
# used in the R interface is the ggplot2 package


# When h2o.explain() is provided a list of models, the following global explanations
# will be generated by default:
#  * Leaderboard (compare all models)
#  * Confusion Matrix for Leader Model (classification only)
#  * Residual Analysis for Leader Model (regression only)
#  * Variable Importance of Top Base (non-Stacked) Model
#  * Variable Importance Heatmap (compare all non-Stacked models)
#  * Model Correlation Heatmap (compare all models)
#  * SHAP Summary of Top Tree-based Model (TreeSHAP)
# * Partial Dependence (PD) Multi Plots (compare all models)
# * Individual Conditional Expectation (ICE) Plots


# Global explanations -- we explain the innerworkings of the model as a whole.

# Explain leader model & compare with all AutoML models
exp_automl <- h2o.explain(automl, valid)
print(exp_automl)


# Explain a specific model
GBM_auc <- h2o.get_best_model(automl, algorithm = "GBM", criterion = "auc")
GLM_auc <- h2o.get_best_model(automl, algorithm = "GLM", criterion = "auc")
DRF_auc <- h2o.get_best_model(automl, algorithm = "DRF", criterion = "auc")
DL_auc <- h2o.get_best_model(automl, algorithm = "DeepLearning", criterion = "auc")
XGB_auc <- h2o.get_best_model(automl, algorithm = "XGBoost", criterion = "auc")


exp_GBM <- h2o.explain(GBM_auc, valid) # Explain the best performing GBM model
print(exp_GBM)
# Some explainations concerning the plots:
# The green lines or dots on the PDP show the average response versus feature value with error bars.
# The grey histogram shows the number of instances for each range of feature values.
# For the monthly income plot plot, we can see that the average leaving rate decreases
# with the montly income, but also that the data becomes increasingly sparse for larger interest rates.


# Local explanations -- Explain a specific unit (i.e. obtain explanations for
# a single person in the sample).

# In the following step, we explain the behavior of a model or group of models
# with respect to a single row of data. By using the h2o.explain_row() function,
# the outputs would be:
#  * SHAP Contribution Plot (for the top tree-based model in AutoML)
#  * Individual Conditional Expectation (ICE) Plots

# In the next line, we explain how the model performs for a single row (row 4 from
# the validation set)
row_4_GBM_auc <- h2o.explain_row(GBM_auc, valid, row_index = 4)
row_4_GBM_auc
