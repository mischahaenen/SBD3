---
title: "Text Mining Groupwork 2: Analysis of Twitter Activities by Swiss Universities of Applied Sciences"
author: "Your Name"
date: "2024-05-18"
output: html_document
---
`
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(textclean)
library(wordcloud)
library(sentimentr)
library(lexicon)
library(topicmodels)
library(tidytext)
library(quanteda)
library(stopwords)
library(syuzhet)
library(textdata)
```
Load the tweets and check if they are loaded correctly
```{r}
# Set working directory
getwd()
setwd("../../data/")

# Load data
load("Tweets_all.rda")

# Check that tweets are loaded
head(tweets)
summary(tweets)
```
Start preprocessing the tweets, to calculate the intervalls some additional properties are needed.
```{r}
# Preprocessing Step: Convert date and time to POSIXct and format according to date, year and university
tweets <- tweets %>%
  mutate(
    created_at = as.POSIXct(created_at, format = "%Y-%m-%d %H:%M:%S"),
    date = as.Date(created_at),
    day = weekdays(created_at),
    day = factor(day, levels = c(
      "Monday", "Tuesday",
      "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"
    )),
    year = year(created_at),
    university = as.character(university)
  )
```

## Question 1: How many tweets are being posted by the various Universities when? Are there any 'release' strategies visible?
```{r}
# Count each tweet by university and hour of the day
tweet_counts_by_hour_of_day <- tweets %>%
  group_by(university, timeofday_hour) %>%
  count() %>%
  arrange(university, timeofday_hour)

# Plot the number of tweets by university and hour of the day
ggplot(
  tweet_counts_by_hour_of_day,
  aes(
    x = timeofday_hour, y = n,
    color = university, group = university
  )
) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and hour",
    x = "Hour of day",
    y = "Number of tweets"
  )

# Show most active hours for each university
hours_with_most_tweets_by_uni <- tweet_counts_by_hour_of_day %>%
  group_by(university, timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(hours_with_most_tweets_by_uni)

# Show most active hour overall
hour_with_most_tweets <- tweet_counts_by_hour_of_day %>%
  group_by(timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  arrange(desc(total_tweets)) %>%
  slice(1)

print(hour_with_most_tweets)

# Count each tweet by university and weekday
tweet_counts_by_week_day <- tweets %>%
  group_by(university, day) %>%
  count() %>%
  arrange(university, day)

# Plot the number of tweets by university and day of the week
ggplot(tweet_counts_by_week_day, aes(
  x = day,
  y = n, color = university,
  group = university
)) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and day of the week",
    x = "Day of the week", y = "Number of tweets"
  )

# Show most active days for each university
days_with_most_tweets_by_uni <- tweet_counts_by_week_day %>%
  group_by(university, day) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(days_with_most_tweets_by_uni)

# Calculate time intervals between tweets
find_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

tweets <- tweets %>%
  arrange(university, created_at) %>%
  group_by(university) %>%
  mutate(time_interval = as.numeric(
    difftime(created_at, lag(created_at), units = "mins")
  ))

# Descriptive statistics of time intervals
summary(tweets$time_interval)

setwd("../4.Text-Mining-Groupwork/plots")
unique_years <- tweets$year %>% unique()
# Plot distribution of time intervals between tweets for each year
for (curr_year in unique_years) {
  # Filter data for the specific year
  filtered_data <- tweets %>%
    filter(year(created_at) == curr_year)

  p <- ggplot(filtered_data, aes(x = time_interval)) +
    geom_histogram(fill = "lightblue") +
    facet_wrap(~university) +
    labs(
      title = paste0(
        "Distribution of time intervals between tweets - ", curr_year
      ),
      x = "Time interval (minutes)",
      y = "Frequency"
    )

  ggsave(filename = paste0("time_interval_plot_", curr_year, ".png"))
  print(p)
  universities <- filtered_data$university %>% unique()
  for (uni in universities) {
    # Filter data for the specific university
    uni_filtered_data <- filtered_data %>%
      filter(university == uni)
    # Calculate mode (most common interval) in hours
    most_common_interval_minutes <- find_mode(uni_filtered_data$time_interval)
    most_common_interval_hours <- most_common_interval_minutes / 60
    print(paste0(
      "Most common time interval for ", uni,
      " in ",
      curr_year,
      " is ", most_common_interval_minutes,
      " minutes (", most_common_interval_hours, " hours)"
    ))
  }
}
```

## 2. What are the tweets about and how do other Twitter users react to them (likes, etc.)?

### Load the data and make previous changes undone
```{r}
getwd()
setwd("./data/")
load("Tweets_all.rda")
```
### Data Preprocessing
```{r}
# Removes: URLS, Emojis, Punctuation, Numbers
tokens <- tokens(tweets$full_text,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE
)
# Remove 'amp' as it is not meaningful because its only & symbol
# Remove 'rt' because it is an word e.g 'engagiert'.
extended_stopwords <- c(
  stopwords("english"),
  stopwords("french"),
  stopwords("german"),
  stopwords("italian"),
  "#fhnw", "#bfh", "@htw_chur", "#hslu", "#supsi",
  "amp", "rt"
)
# transform to lowercase
tokens <- tokens_tolower(tokens)
# Stem all words
tokens <- tokens_wordstem(tokens)
# Create n-grams of any length (including bigrams and trigrams)
tokens <- tokens_ngrams(tokens, n = 1)
# remove stopwords im multiple languages and remove university hashtags
tokens <- quanteda::tokens_select(tokens,
  pattern = extended_stopwords,
  selection = "remove"
)
# Create Document-feature-matrix
doc_matrix <- dfm(tokens)
```
### Content Analysis
```{r}
# Word Frequencies
word_freqs <- doc_matrix %>%
  colSums() %>%
  sort(decreasing = TRUE)

# Top 20 words
head(word_freqs, 20)

word_freqs_df <- data.frame(
  word = featnames(doc_matrix),
  freq = colSums(doc_matrix)
)

# Create the word cloud
set.seed(1234)
wordcloud(
  words = word_freqs_df$word,
  freq = word_freqs_df$freq,
  min.freq = 5,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.35,
  colors = brewer.pal(8, "Dark2")
)
```
### Userreaction Analysis
```{r}
# Identify the tweets with the most likes
most_liked_tweets <- tweets %>%
  arrange(desc(favorite_count)) %>%
  head(1000)

# Analyze the posting time of the most liked tweets
most_liked_tweets_time <- most_liked_tweets %>%
  mutate(time_of_day = format(created_at, "%H"))

# Plot the distribution of the posting times
ggplot(most_liked_tweets_time, aes(x = as.numeric(time_of_day))) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "blue") +
  labs(
    title = "Distribution of Posting Times for Most Liked Tweets",
    x = "Hour of Day",
    y = "Frequency"
  )
```
Analyse the content of the most liked tweets
```{r}
# Preprocessing content of most liked tweets
most_liked_tokens <- tokens(most_liked_tweets$full_text,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE
)

# Apply the same stopword removal and transformations
most_liked_tokens <- tokens_select(most_liked_tokens,
  pattern = extended_stopwords, selection = "remove"
)

most_liked_tokens <- tokens_tolower(most_liked_tokens)
most_liked_tokens <- tokens_wordstem(most_liked_tokens)
most_liked_tokens <- tokens_ngrams(most_liked_tokens, n = 1:3)
most_liked_doc_matrix <- dfm(most_liked_tokens)

# Word Frequencies of Most Liked Tweets
most_liked_word_freqs <- most_liked_doc_matrix %>%
  colSums() %>%
  sort(decreasing = TRUE)

# Create a word cloud for most liked tweets
most_liked_word_freqs_df <- data.frame(
  word = featnames(most_liked_doc_matrix),
  freq = colSums(most_liked_doc_matrix)
)

set.seed(1234)
wordcloud(
  words = most_liked_word_freqs_df$word,
  freq = most_liked_word_freqs_df$freq,
  min.freq = 2,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.35,
  colors = brewer.pal(8, "Dark2")
)
```
## Question 3: How do the university tweets differ in terms of content, style, emotions, etc?

### Content Analysis (Word Clouds)
```{r}
for (uni in unique(tweets$university)) {
  uni_tokens <- tokens(tweets$full_text,
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE,
    remove_separators = TRUE
  )
  uni_tokens <- tokens_select(uni_tokens,
    pattern = extended_stopwords, selection = "remove"
  )
  # transform to lowercase
  uni_tokens <- tokens_tolower(uni_tokens)
  # Stem all words
  uni_tokens <- tokens_wordstem(uni_tokens)
  # Create n-grams of any length (including bigrams and trigrams)
  uni_tokens <- tokens_ngrams(uni_tokens, n = 1:3)
  uni_dfm <- dfm(uni_tokens)

  uni_word_freqs_df <- data.frame(
    word = featnames(uni_dfm),
    freq = colSums(uni_dfm)
  )

  # Create the word cloud
  set.seed(1234)
  wordcloud(
    words = uni_word_freqs_df$word,
    freq = uni_word_freqs_df$freq,
    min.freq = 5,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(8, "Dark2")
  )
  # TODO: Save wordcloud
}

bi_gram_tokens <- tokens_ngrams(tokens, n = 2)
bi_gram_matrix <- dfm(bi_gram_tokens)

bi_gram_freqs_df <- data.frame(
  word = featnames(bi_gram_matrix),
  freq = colSums(bi_gram_matrix)
)

# Create the bigram word cloud
set.seed(1235) # For reproducibility
wordcloud(
  words = bi_gram_freqs_df$word,
  freq = bi_gram_freqs_df$freq,
  min.freq = 3,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.35,
  colors = brewer.pal(8, "Accent")
)

# Trigram Wordcloud
tri_gram_tokens <- tokens_ngrams(tokens, n = 3)
tri_gram_matrix <- dfm(tri_gram_tokens)

tri_gram_freqs_df <- data.frame(
  word = featnames(tri_gram_matrix),
  freq = colSums(tri_gram_matrix)
)

# Create the trigram word cloud
set.seed(1236) # For reproducibility
wordcloud(
  words = tri_gram_freqs_df$word,
  freq = tri_gram_freqs_df$freq,
  min.freq = 2,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0,
  colors = brewer.pal(8, "Paired")
)

# TODO: Custom Dictionary?
```
### LDA Topic Modeling
```{r}
# TODO: Check why word_matric takes forever and doc_matrix does not work
tweet_lda <- LDA(word_freqs, k = 5, control = list(seed = 1237))
# Tidy the LDA results
topic_terms <- tidy(tweet_lda, matrix = "beta")
# Extract topics and top terms
topics <- as.data.frame(terms(tweet_lda, 50)) # First fifty words per topic

# Tidy the LDA results
tweet_lda_td <- tidy(tweet_lda)

# Extract top terms per topic
top_terms <- tweet_lda_td %>%
  group_by(topic) %>%
  top_n(8, beta) %>% # Show top 8 terms per topic
  ungroup() %>%
  arrange(topic, -beta)

# Visualize top terms per topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered() +
  labs(
    x = "Beta (Term Importance within Topic)",
    y = NULL,
    title = "Top Terms per Topic in Tweets (LDA)"
  )

# Most different words among topics (using log ratios)
diff <- tweet_lda_td %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001) %>%
  mutate(
    logratio_t1t2 = log2(topic2 / topic1),
    logratio_t1t3 = log2(topic3 / topic1),
    logratio_t2t3 = log2(topic3 / topic2)
  )
diff

# Add topic probabilities to original data
lda_gamma <- tidy(tweet_lda, matrix = "gamma")
tweets <- tweets %>%
  mutate(document_id = row_number()) %>% # Add a unique ID for each tweet
  left_join(lda_gamma, by = c("document_id" = "document"))

# Analyze topics by university
tweets %>%
  count(university, topic, wt = gamma) %>%
  group_by(university) %>%
  slice_max(n = 3, order_by = n, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic)) %>%
  ggplot(aes(x = university, y = n, fill = topic)) +
  geom_col(position = "dodge") +
  labs(title = "Top 3 Topics by University", y = "Topic Proportion")
```
### Style Analysis
```{r}
tweets %>%
  mutate(tweet_length = nchar(full_text)) %>%
  ggplot(aes(x = tweet_length)) +
  geom_histogram() +
  labs(title = "Distribution of Tweet Lengths")
```
### Sentiment Analysis
```{r}
# Sentiment over Time (Assuming you have a 'created_at' column for dates)
tweets$date <- as.Date(tweets$created_at)
tweets$month <- month(tweets$created_at)

# Calculate Sentiment
# TODO: Ask if there is a specific sentiment lexicon to use and if by sentence is enough. I think by word is better as in line 385
tweets$sentiment <- get_sentiment(tweets$full_text,
  method = "syuzhet", lang = "german"
)

# Load NRC lexicon
nrc_lexicon <- get_sentiments("nrc")

# Calculate Sentiment (Syuzhet and NRC)
sentiment_tokens <- tweets %>%
  select(id, full_text) %>%
  unnest_tokens(word, full_text) %>%
  inner_join(nrc_lexicon) %>%
  mutate(
    sentiment_syuzhet = get_sentiment(word,
      method = "syuzhet",
      lang = "german"
    ),
    sentiment_positive = ifelse(sentiment == "positive", 1, 0),
    sentiment_negative = ifelse(sentiment == "negative", 1, 0)
  )

# Aggregate sentiment back to the tweet level
tweet_sentiment <- sentiment_tokens %>%
  group_by(id) %>%
  summarize(
    mean_sentiment_syuzhet = mean(sentiment_syuzhet, na.rm = TRUE),
    total_positive = sum(sentiment_positive, na.rm = TRUE),
    total_negative = sum(sentiment_negative, na.rm = TRUE)
  )

# Join sentiment back to the main dataframe
tweets <- tweets %>%
  left_join(tweet_sentiment, by = "id")

# Sentiment over Time
plot_data <- tweets %>%
  group_by(tweet_month) %>%
  summarize(
    mean_sentiment_syuzhet = mean(mean_sentiment_syuzhet, na.rm = TRUE),
    mean_positive = mean(total_positive, na.rm = TRUE),
    mean_negative = mean(total_negative, na.rm = TRUE)
  )

# Plot Syuzhet sentiment
ggplot(plot_data, aes(x = tweet_month, y = mean_sentiment_syuzhet)) +
  geom_line() +
  labs(
    title = "Mean Syuzhet Sentiment Over Time",
    y = "Mean Sentiment Score"
  ) +
  scale_x_date(date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot NRC positive and negative sentiments
ggplot(plot_data, aes(x = tweet_month)) +
  geom_line(aes(y = mean_positive, color = "Positive")) +
  geom_line(aes(y = mean_negative, color = "Negative")) +
  labs(
    title = "Mean NRC Positive & Negative Sentiment Over Time",
    y = "Mean Sentiment Score"
  ) +
  scale_x_date(date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Sentiment by University
tweets %>%
  ggplot(aes(x = university, y = mean_sentiment_syuzhet, fill = university)) +
  geom_boxplot() +
  labs(title = "Sentiment Comparison Across Universities (Syuzhet)")
```

## Question 4: What specific advice can you give us as communication department of BFH based on your analysis? How can we integrate the analysis of tweets in our internal processes, can you think of any data products that would be of value for us?

### Summary key insights from the analysis
```{r}
insights <- list(
  "Most Active Hours" = hours_with_most_tweets_by_uni,
  "Most Active Days" = days_with_most_tweets_by_uni,
  "Most Common Time Intervals" = most_common_interval_minutes,
  "Content Analysis" = word_freqs_df,
  "Sentiment Analysis" = tweet_sentiment
)
```
# Recommendations:
# 1. Optimize tweet release times based on peak engagement hours.
# 2. Focus on specific days with high activity for important announcements.
# 3. Utilize sentiment analysis to tailor content that resonates positively with the audience.
# 4. Implement topic modeling to identify key themes and align communication strategy accordingly.

