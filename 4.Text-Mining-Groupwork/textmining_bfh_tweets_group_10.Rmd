---
title: "Text Mining Groupwork 2: Analysis of Twitter Activities by Swiss Universities of Applied Sciences"
author: "Fabio Burri, Mischa Haenen, Robin Galeazzi and Timon Galeazzi"
date: "2024-05-20"
output: html_document
---
`
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(textclean)
library(wordcloud)
library(sentimentr)
library(lexicon)
library(topicmodels)
library(tidytext)
library(quanteda)
library(stopwords)
library(syuzhet)
library(textdata)
library(cld3)
library(tidyEmoji)
library(dplyr)
library(reshape2)
library(entropy)
```
Load the tweets and check if they are loaded correctly. We also check the summary for a first interpretation. The summary(tweets) output reveals the following:

- Time Range: Tweets span from 2009 to 2023, with most posted between 2015 and 2020.
- Distribution: The data is unevenly distributed, with the median date in 2018.
- Engagement: Retweets and favorites range from 0 to 267 and 188, respectively, with most tweets having low engagement.
- Universities: The university column indicates tweets from multiple universities, allowing for comparison.

```{r}
# Set working directory
# getwd()
# setwd("./data/")

# Load data
load("./Tweets_all.rda")

# Check that tweets are loaded
head(tweets)
summary(tweets)
```
Start preprocessing the tweets, to calculate the intervalls some additional properties are needed. The preprocessing steps transform raw tweet data into a structured format suitable for analysis. This includes:

- Standardizing Date and Time: Ensures accurate temporal analysis.
- Extracting Features: Derives relevant information (year, month, day, language) for targeted analysis.
- Handling Emojis: Converts emojis to text for sentiment analysis and visualization.
- Data Cleaning: Removes unnecessary tags and standardizes data types.
```{r}
# Preprocessing Step: Convert date and time to POSIXct and format according to date, year and university. Detect language and extract emojis. The days are sorted from the system locale starting from monday
tweets <- tweets %>%
  mutate(
    created_at = as.POSIXct(created_at, format = "%Y-%m-%d %H:%M:%S"),
    date = as.Date(created_at),
    day = lubridate::wday(created_at,
      label = TRUE, abbr = FALSE,
      week_start = getOption("lubridate.week.start", 1),
      locale = Sys.getlocale("LC_TIME")
    ),
    year = year(created_at),
    month = floor_date(created_at, "month"),
    university = as.character(university),
    full_text_emojis = replace_emoji(full_text, emoji_dt = lexicon::hash_emojis)
  )

# Remove Emoji Tags helper funciton
# replace emoji places the emojis in the text as tags and their name, we remove them here
remove_emoji_tags <- function(text) {
  str_remove_all(text, "<[a-z0-9]{2}>")
}
# Remove Emoji Tags
tweets$full_text_emojis <- sapply(tweets$full_text_emojis, remove_emoji_tags)

# Store emojis in a sep arate column to analyze later
tweets$emoji_unicode <- tweets %>%
  emoji_extract_nest(full_text) %>%
  select(.emoji_unicode)
```

## Question 1: How many tweets are being posted by the various Universities when? Are there any 'release' strategies visible?

### Most Active Hours:
Each university has a distinct peak hour for tweeting, often aligning with typical working hours (9 AM - 5 PM). This suggests a strategic approach to reach their target audience when they are most likely online. The most active hours for each university are as follows:

- FHNW: 9 AM
- FH Graubünden: 11 AM
- ZHAW: 5 PM
- BFH: 8 AM
- HES-SO: 10 AM
- HSLU: 9 AM
- OST-FH: 8 AM
- SUPSI-CH: 11 AM

These times typically align with standard working hours, indicating a strategic approach to reach their audience during times they are most likely to be online. It appears that a typical worker is more productive and active on Twitter in the morning, with motivation waning around midday and continuing to decline until the end of the workday.

### Most Active Days:
There isn't a consistent "most active day" across universities. Some favor weekdays, while others show higher activity on weekends. This could reflect differences in their target audience or the nature of their content.

- FHNW: Tuesday
- FH Graubünden: Tuesday
- ZHAW: Wednesday
- BFH: Tuesday
- HES-SO: Tuesday
- HSLU: Thursday
- OST-FH: Friday
- SUPSI-CH: Friday

The pattern also suggests that tweet activity tends to be higher earlier in the week, with motivation and tweet frequency potentially falling as the week progresses.

```{r}
# Count each tweet by university and hour of the day
tweet_counts_by_hour_of_day <- tweets %>%
  group_by(university, timeofday_hour) %>%
  count() %>%
  arrange(university, timeofday_hour)

# Plot the number of tweets by university and hour of the day
ggplot(
  tweet_counts_by_hour_of_day,
  aes(
    x = timeofday_hour, y = n,
    color = university, group = university
  )
) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and hour",
    x = "Hour of day",
    y = "Number of tweets"
  )

# Show most active hours for each university
hours_with_most_tweets_by_uni <- tweet_counts_by_hour_of_day %>%
  group_by(university, timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(hours_with_most_tweets_by_uni)

# Show most active hour overall
hour_with_most_tweets <- tweet_counts_by_hour_of_day %>%
  group_by(timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  arrange(desc(total_tweets)) %>%
  slice_max(n = 1, order_by = total_tweets)

print(hour_with_most_tweets)

# Count each tweet by university and weekday
tweet_counts_by_week_day <- tweets %>%
  group_by(university, day) %>%
  count() %>%
  arrange(university, day)

# Plot the number of tweets by university and day of the week
ggplot(
  tweet_counts_by_week_day,
  aes(
    x = day, y = n,
    color = university,
    group = university
  )
) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and day of the week",
    x = "Day of the week",
    y = "Number of tweets"
  )

# Show most active days for each university
days_with_most_tweets_by_uni <- tweet_counts_by_week_day %>%
  group_by(university, day) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(days_with_most_tweets_by_uni)

# Combine the most active hours and days for each university to show heatmap
heatmap_data <- tweets %>%
  group_by(timeofday_hour, day) %>%
  count() %>%
  ungroup()

# Plot heatmap and we can see clearly that the most tweets are posted during the working hours from monday to friday
ggplot(heatmap_data, aes(x = day, y = timeofday_hour, fill = n)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(
    title = "Heatmap of Tweet Activity by Hour and Day",
    x = "Day",
    y = "Hour",
    fill = "Number of Tweets"
  ) +
  theme_minimal()
```

### Release Strategies:
While universities have peak hours and days, the intervals between tweets vary significantly, indicating a more reactive strategy rather than a rigid release schedule. This variability suggests that the universities might be responding to real-time events or trends rather than sticking to a strict posting schedule.

#### Dispersion Measures and Analysis
To further understand the dispersion of tweets, we analyzed the time intervals between tweets using measures like the mean interval, standard deviation, and entropy. Understanding the variability helps in assessing how consistent or sporadic the posting behavior is. High variability indicates that the university does not follow a strict schedule and posts at irregular intervals, which could be a sign of a more reactive approach to social media. Higher entropy suggests less predictability in tweet timing, indicating a more dynamic and responsive posting strategy. This is crucial for understanding how universities might be reacting to real-time events or trends rather than following a predetermined schedule.Here are the results for selected universities:

- **FHNW**: Significant variability in mean intervals (1291 to 6994 minutes) and stable entropy with higher values in some years, indicating fluctuating engagement and reactive posting at times.
- **FH Graubünden**: High variability in mean intervals, notably increasing in 2023 (13889 minutes). Entropy peaks in 2021 (2.78), suggesting a highly reactive posting strategy.
- **ZHAW**: Fluctuating mean intervals, with a significant increase in 2023 (35371 minutes). High entropy in 2020 (2.38), indicating dynamic and reactive posting periods.
- **BFH**: Generally stable mean intervals with minor fluctuations. Entropy shows varying predictability, indicating a mix of stable and responsive tweeting.
- **HES-SO**: High variability in mean intervals, especially in earlier years. Generally stable entropy with some fluctuations, indicating a mix of consistent and reactive posting.
- **HSLU**: Fluctuating mean intervals with a decrease in 2021. Entropy indicates varying unpredictability, suggesting a mix of consistent and reactive posting.
- **OST-FH**: Higher mean intervals in 2021 and 2022, indicating less frequent posting. High variability in entropy, particularly in 2022, suggesting flexible posting behavior.
- **SUPSI-CH**: Significant fluctuations in mean intervals over the years. High entropy in 2020, indicating a dynamic and reactive posting strategy.

```{r}
tweets <- tweets %>%
  arrange(university, created_at) %>%
  group_by(university) %>%
  mutate(time_interval = as.numeric(
    difftime(created_at, lag(created_at), units = "mins")
  ))

# Plotting the time intervals
universities <- unique(tweets$university)
for (uni in universities) {
  uni_filtered_data <- tweets %>%
    filter(university == uni)

  # Plot the distribution of time intervals
  print(ggplot(uni_filtered_data, aes(x = time_interval)) +
    geom_histogram(fill = "lightblue", bins = 30) +
    facet_wrap(~year) +
    labs(
      title = paste0("Distribution of time intervals between tweets - ", uni),
      x = "Time interval (minutes)",
      y = "Tweet count"
    ))

  # Plot posting day for each year for university because a intervall could be "short" but when a university only posts twice a year it seems active but it is actually not
  tweet_counts <- uni_filtered_data %>%
    group_by(tweet_month) %>%
    summarise(tweet_count = n())
  print("Tweet count for each month")
  print(ggplot(tweet_counts, aes(x = tweet_month, y = tweet_count)) +
    geom_line(color = "#F9C301") +
    geom_point(color = "#37556E", size = 3) +
    scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
    labs(
      title = "Monthly Tweet Activity",
      x = "Month",
      y = "Tweet Count"
    ) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)))

  # Calculate dispersion measures for time intervals
  dispersion_measures <- uni_filtered_data %>%
    group_by(year) %>%
    summarise(
      mean_interval = mean(time_interval, na.rm = TRUE),
      sd_interval = sd(time_interval, na.rm = TRUE),
      entropy_interval = entropy::entropy(discretize(time_interval,
        numBins = 30, r = range(time_interval, na.rm = TRUE)
      ))
    )

  print(paste("Dispersion measures for", uni))
  print(dispersion_measures)

  # Line plot for mean interval and standard deviation over the years
  print(ggplot(dispersion_measures, aes(x = year)) +
    geom_line(aes(y = mean_interval, color = "Mean Interval")) +
    geom_point(aes(y = mean_interval, color = "Mean Interval")) +
    geom_line(aes(y = sd_interval, color = "Standard Deviation")) +
    geom_point(aes(y = sd_interval, color = "Standard Deviation")) +
    scale_y_continuous(sec.axis = dup_axis()) +
    labs(
      title = "Mean Interval and Standard Deviation of Tweet Intervals Over Years",
      x = "Year",
      y = "Minutes",
      color = "Measure"
    ) +
    theme_minimal())

  print(ggplot(dispersion_measures, aes(x = year)) +
    geom_line(aes(y = entropy_interval, color = "Entropy Interval")) +
    geom_point(aes(y = entropy_interval, color = "Entropy Interval")) +
    scale_y_continuous(sec.axis = dup_axis()) +
    labs(
      title = "Entropy of Tweet Intervals Over Years",
      x = "Year",
      y = "Entropy"
    ) +
    theme_minimal())
}

# Descriptive statistics of time intervals
summary(tweets$time_interval)
```
### Conclusion:
The data indicates that Swiss Universities of Applied Sciences primarily tweet during working hours and show distinct patterns in their most active days and hours. Workers tend to be more productive and active on Twitter in the morning, with a noticeable decline in activity around midday and towards the end of the week. The dispersion measures support this interpretation, showing varying levels of unpredictability across different years and universities.

## Question 2: What are the tweets about and how do other Twitter users react to them (likes, etc.)?

### Data Preprocessing
The tweets are filtered based on language, focusing on German, French, Italian, and English. These languages where choosen based on the popularity over all tweet languages. It removes common and extended stopwords, including non-meaningful words like 'amp' (which represents '&') and 'rt' (commonly found in retweets). The extended stopwords list includes hashtags and URLs related to specific Swiss universities.

Next, the code processes tweets separately for each language. This involves creating tokens from the text, removing unwanted characters, stemming words, and creating n-grams. The processed tokens are then used to create Document-Feature Matrices (DFMs) for each language.
```{r}
langs <- c("de", "fr", "it", "en")
tweets_filtered <- tweets %>%
  filter(lang %in% langs)

# Define extended stopwords (outside loop for efficiency)
# Remove 'amp' as it is not meaningful because its only & symbol
# Remove 'rt' because it is an word e.g 'engagiert'.
extended_stopwords <- c(
  "#fhnw", "#bfh", "@htw_chur", "#hslu", "#supsi", "#sups",
  "amp", "rt", "fr", "ber", "t.co", "https", "http", "www", "com", "html"
)
# Create separate DFMs for each language
dfm_list <- list()
for (sel_lang in langs) {
  # Subset tweets for the current language
  tweets_lang <- tweets_filtered %>%
    filter(lang == sel_lang)
  # Create tokens for the current language
  stopwords_lang <- stopwords(sel_lang)
  # Create tokens for all tweets:
  # - create corpus and tokens because tokensonly works on character, corpus, list, tokens, tokens_xptr objects.
  # - create tokens and remove: URLS, Punctuation, Numbers, Symbols, Separators
  # - transform to lowercase
  # - Stem all words
  # - Create n-grams of any length (not includinf bigrams and trigrams but they are shown later)
  # - It is important to remove the stopwords after stemming the words because we remove the endings from some stem words
  tokens_lang <- tweets_lang %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem(lang = sel_lang) %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(stopwords_lang, extended_stopwords), selection = "remove"
    )
  # Create DFM for the current language
  dfm_list[[sel_lang]] <- dfm(tokens_lang)
}
```
### Content Analysis
Tweets were analyzed across four languages: German, French, Italian, and English. Each university tends to tweet predominantly in one or more languages, reflecting the linguistic diversity of Switzerland.

- German: Predominantly used by BFH and FHNW. Common words include "neu" (new), "mehr" (more), "schweiz" (Switzerland), and "studier" (study).
- French: Primarily used by HES-SO. Common words include "projet" (project), "recherch" (research), and "tudi" (study).
- Italian: Mostly used by SUPSI. Common words include "nuov" (new), "progett" (project), and "student" (student).
- English: Frequently used by HSLU. Common words include "student", "project", "thank", and "university".
### Word Frequency:
- English: The most frequent words were "student", "new", "@hslu", "university", "project", "thank", "@zhaw", "day", "science", and "today".
- German: The most frequent words were "neu", "mehr", "schweiz", "werd", "all", "studier", "heut", "hochschul", "bfh", and "jahr".
- Italian: The most frequent words were "nuov", "sups", "progett", "student", "present", "info", "iscrizion", "cors", "ricerc", and "formazion".
- French: The most frequent words were "hes-so", "right", "arrow", "dan", "projet", "a", "tudi", "haut", "col", and "@hes_so".

It's important to note that some words like "right" 👉 and "arrow" ➡️ are actually names of parsed emojis and not written words in the tweets.

Word clouds for each language visually show the most common words, emphasizing their relative frequencies. The analysis revealed that universities tweet in multiple languages, reflecting the linguistic diversity of their audience. But still we can order the universities by language. For example BFH and FHNW are tweeting in german, HES-SO in french, SUPSI in italian and HSLU in english.

```{r}
# Word Frequencies & Visualization
words_freqs_en <- sort(colSums(dfm_list$en), decreasing = TRUE)
head(words_freqs_en, 20)
wordcloud(
  words = names(words_freqs_en),
  freq = words_freqs_en,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)

words_freqs_de <- sort(colSums(dfm_list$de), decreasing = TRUE)
head(words_freqs_de, 20)
wordcloud(
  words = names(words_freqs_de),
  freq = words_freqs_de,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)

words_freqs_it <- sort(colSums(dfm_list$it), decreasing = TRUE)
head(words_freqs_it, 20)
wordcloud(
  words = names(words_freqs_it),
  freq = words_freqs_it,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)

# It seems that there are some english words but I think this are emojis
words_freqs_fr <- sort(colSums(dfm_list$fr), decreasing = TRUE)
head(words_freqs_fr, 20)
wordcloud(
  words = names(words_freqs_fr),
  freq = words_freqs_fr,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)
# University-specific Analysis
for (uni in unique(tweets$university)) {
  # Subset tweets for the current language
  uni_tweets <- tweets %>%
    filter(university == uni)

  tokens_lang <- uni_tweets %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem() %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(
          stopwords("en"), stopwords("de"),
          stopwords("fr"), stopwords("it"), extended_stopwords
        ), selection = "remove"
    )
  # Create Data Frame Matrix for uni with all languages
  uni_dfm <- dfm(tokens_lang)
  # Word Frequencies
  uni_word_freqs <- sort(colSums(uni_dfm), decreasing = TRUE)
  # print most common words: the emoji right are used often
  print(paste("Most common words for", uni, ":"))
  print(head(uni_word_freqs, 20))
  wordcloud(
    words = names(uni_word_freqs),
    freq = uni_word_freqs,
    max.words = 200,
    random.order = FALSE,
    colors = brewer.pal(8, "Dark2")
  )
}
```

### Userreaction Analysis
To understand user reactions, the code calculates a 'weighted engagement' metric, combining favorite and retweet counts. The tweets with the highest engagement are analyzed by hour and day to identify patterns in user interaction.

### Emgagement Time of Most Engaged Tweets:

The provided bar plots show the average engagement of tweets by hour of the day and by day of the week. Each bar represents the average engagement score for tweets posted during specific hours or on specific days.

#### Peak Engagement:

- The highest average engagement occurs around 20:00 (8 PM), with a significant spike.
- Other notable peaks are at 16:00 (4 PM) and 19:00 (7 PM), showing elevated engagement during these hours.
- Tuesday shows the highest average engagement, indicating that tweets posted on this day receive the most interaction from users.
- Friday and Sunday also have relatively high engagement, suggesting that these days are good for posting tweets.

#### Consistent Engagement:

- Engagement remains relatively consistent throughout the day from 06:00 (6 AM) to 17:00 (5 PM), with moderate fluctuations.
- Monday and Thursday show consistent engagement scores that are moderate compared to the peak days. Engagement on these days is lower than Tuesday but higher than Wednesday and Saturday.

#### Lower Engagement:

- There is a noticeable dip in engagement early in the morning around 00:00 (midnight) and 23:00 (11 PM).
- Engagement is also relatively lower around 22:00 (10 PM).
- Wednesday and Saturday have the lowest average engagement scores, indicating that tweets posted on these days receive less interaction compared to other days.

```{r}
# Calculate a 'weighted engagement' metric
tweets <- tweets %>%
  mutate(
    weighted_engagement = favorite_count * 1 + retweet_count * 2
  )

# Identify tweets with the highest weighted engagement
most_engaged_tweets <- tweets %>%
  arrange(desc(weighted_engagement)) %>%
  head(1000) # Top 1000 for analysis

# Calculate average engagement by hour
engagement_hour <- most_engaged_tweets %>%
  group_by(timeofday_hour) %>%
  summarise(avg_engagement = mean(weighted_engagement, na.rm = TRUE))

ggplot(engagement_hour, aes(x = timeofday_hour, y = avg_engagement)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(
    title = "Average Engagement by Hour",
    x = "Hour of Day",
    y = "Average Engagement"
  ) +
  theme_minimal()

# Calculate average engagement by day
engagement_day <- most_engaged_tweets %>%
  group_by(day) %>%
  summarise(avg_engagement = mean(weighted_engagement, na.rm = TRUE))

# Plot average engagement by day
ggplot(engagement_day, aes(x = day, y = avg_engagement)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(
    title = "Average Engagement by Day of the Week",
    x = "Day of the Week",
    y = "Average Engagement"
  ) +
  theme_minimal()
```

### Analyse the content of the most liked tweets
The most common words in the most liked tweets include "mehr" (more), "neue" (new), "schweiz" (Switzerland), "heut" (today), and "hochschul" (university). These words suggest that tweets focusing on new developments, events happening today, and general updates about Switzerland and universities tend to receive more likes.
```{r}
# Preprocessing content of most liked tweets
tokens_most_engaged <- most_engaged_tweets %>%
  corpus(text_field = "full_text_emojis") %>%
  tokens(
    remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
    remove_url = TRUE, remove_separators = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_wordstem(lang = sel_lang) %>%
  tokens_ngrams(n = 1) %>%
  tokens_select(
    pattern =
      c(
        stopwords("en"), stopwords("de"),
        stopwords("fr"), stopwords("it"), extended_stopwords
      ), selection = "remove"
  )
tokens_most_engaged_dfm <- dfm(tokens_most_engaged)
freqs_most_engaged <- sort(colSums(tokens_most_engaged_dfm), decreasing = TRUE)
# print most common words: the emoji right are used often
head(freqs_most_engaged, 20)
set.seed(123)
wordcloud(
  words = names(freqs_most_engaged),
  freq = freqs_most_engaged,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)
```

## Question 3: How do the university tweets differ in terms of content, style, emotions, etc?

### Content Analysis (Word Clouds)
Each university shows distinct patterns in the words and emojis used in their tweets. The analysis involved creating word clouds and identifying the most common words and emojis.

Most Common Words:

- FHNW: Common words include "mehr" (more), "hochschul" (university), and "studierend" (students).
- FH Graubünden: Words like "chur" (location), "htw" (university abbreviation), and "busi" (business) are frequent.
- ZHAW: Frequent words include "zhaw" (university abbreviation), "engineering", and "schweizer" (Swiss).
- BFH: Common terms are "bern" (location), "projekt" (project), and "hochschul" (university).
- HES-SO: Words like "hes-so" (university abbreviation), "projet" (project), and "tudiant" (student) are prevalent.
- HSLU: Common words are "hslu" (university abbreviation), "luzern" (location), and "hochschul" (university).
- OST-FH: Frequent terms include "ost" (university abbreviation), "st.gallen" (location), and "podcast".
- SUPSI-CH: Words like "supsi" (university abbreviation), "formazion" (education), and "progetto" (project) are prevalent.

Most Common Emojis:
- FHNW: Top emojis include 👉 (backhand index pointing right), 💛 (yellow heart), and 🖤 (black heart).
- FH Graubünden: Frequent emojis are 🎉 (party popper), 😃 (grinning face with big eyes), and 😊 (blush).
- ZHAW: Common emojis include 👉 (backhand index pointing right), ⚡ (high voltage), and 😉 (wink).
- BFH: Top emojis are 👉 (backhand index pointing right), 🔋 (battery), and 👇 (backhand index pointing down).
- HES-SO: Common emojis are 👉 (backhand index pointing right), 🎓 (graduation cap), and ➡ (arrow right).
- HSLU: Top emojis include 🎓 (graduation cap), 👨 (man), and 🚀 (rocket).
- OST-FH: Frequent emojis are 👉 (backhand index pointing right), ➡ (arrow right), and 🎓 (graduation cap).
- SUPSI-CH: Common emojis include 👉 (backhand index pointing right), 🎓 (graduation cap), and 🎉 (party popper).
```{r}
for (uni in unique(tweets$university)) {
  uni_tweets <- tweets %>%
    filter(university == uni, lang %in% langs)
  tokens_uni <- uni_tweets %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem() %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(
          stopwords("en"), stopwords("de"),
          stopwords("fr"), stopwords("it"), extended_stopwords
        ), selection = "remove"
    )
  uni_dfm <- dfm(tokens_uni)
  freqs_uni <- sort(colSums(uni_dfm), decreasing = TRUE)
  # print most common words: the emoji right are used often
  print(paste("Most common words for", uni, ":"))
  print(head(freqs_uni, 20))
  set.seed(123)
  wordcloud(
    words = names(freqs_uni),
    freq = freqs_uni,
    max.words = 200,
    random.order = FALSE,
    colors = brewer.pal(8, "Dark2")
  )
  # Analyze Top Emojis by University
  emoji_count_per_university <- uni_tweets %>%
    top_n_emojis(full_text)
  print(paste("Top emojis for", uni, ":"))
  print(emoji_count_per_university)

  emoji_count_per_university %>%
    mutate(emoji_name = reorder(emoji_name, n)) %>%
    ggplot(aes(n, emoji_name)) +
    geom_col() +
    labs(x = "Count", y = NULL, title = "Top 20 Emojis Used")
}
# Generate general tokens for bigram and trigram analysis
tokens <- tweets %>%
  corpus(text_field = "full_text_emojis") %>%
  tokens(
    remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
    remove_url = TRUE, remove_separators = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_wordstem() %>%
  tokens_select(
    pattern =
      c(
        stopwords("en"), stopwords("de"),
        stopwords("fr"), stopwords("it"), extended_stopwords
      ), selection = "remove"
  )
# Bigram Wordcloud
bi_gram_tokens <- tokens_ngrams(tokens, n = 2)
dfm_bi_gram <- dfm(bi_gram_tokens)
freqs_bi_gram <- sort(colSums(dfm_bi_gram), decreasing = TRUE)
head(freqs_bi_gram, 20)
# Create the bigram word cloud
set.seed(123)
wordcloud(
  words = names(freqs_bi_gram),
  freq = freqs_bi_gram,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)

# Trigram Wordcloud
tri_gram_tokens <- tokens_ngrams(tokens, n = 3)
dfm_tri_gram <- dfm(tri_gram_tokens)
reqs_tri_gram <- sort(colSums(dfm_tri_gram), decreasing = TRUE)
head(reqs_tri_gram, 20)
# Create the bigram word cloud
set.seed(123)
wordcloud(
  words = names(reqs_tri_gram),
  freq = reqs_tri_gram,
  max.words = 200,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2")
)
```

### LDA Topic Modeling
The Latent Dirichlet Allocation (LDA) model was applied to the entire dataset of tweets to identify common topics. Here, the model with 5 topics was selected, and the top terms for each topic were extracted.

#### General Topics Identified:

- Topic 1: Includes terms like "student", "project", "innov", "scienc", indicating a focus on academic and research activities.
- Topic 2: Contains words such as "thank", "welcome", "now", "today", highlighting interactions and expressions of gratitude or positive events.
- Topic 3: Features terms like "student", "research", "switzerland", "new", focusing also on academic and research activities.
- Topic 4: Involves terms like "digit", "univers", "day" this handlea also about academic and research activities but also about digitalization.
- Topic 5: Includes words such as "new", "student", "present", "conf", emphasizing new developments, student activities, and events or conferences.

It seems that these topics are not that far apart, which could indicate that the tweets are quite similar in content and style. The LDA model still shows us that topics like academic and research activities, student activities, and new developments are relevant in the tweets.

```{r}
# Source: Christoph Zangger -> löscht alle Reihen mit nur 0s
new_dfm <- dfm_subset(dfm_list$en, ntoken(dfm_list$en) > 0)
tweet_lda <- LDA(new_dfm, k = 5, control = list(seed = 123))
# Tidy the LDA results
topic_terms <- tidy(tweet_lda, matrix = "beta")
# Extract topics and top terms
topics <- as.data.frame(terms(tweet_lda, 50)) # First fifty words per topic

# Extract top terms per topic
top_terms <- topic_terms %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

print(as.data.frame(terms(tweet_lda, 20)))

# Visualize top terms per topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered() +
  labs(
    x = "Beta (Term Importance within Topic)",
    y = "Terms",
    title = "Top Terms per Topic in Tweets (LDA)"
  ) +
  theme_minimal()

# Most different words among topics (using log ratios)
diff <- topic_terms %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001) %>%
  mutate(
    logratio_t1t2 = log2(topic2 / topic1),
    logratio_t1t3 = log2(topic3 / topic1),
    logratio_t2t3 = log2(topic3 / topic2)
  )
diff

# LDA Topic Modeling for each university
universities <- unique(tweets$university)

for (uni in universities) {
  # Filter tweets for the current university
  uni_tweets <- tweets %>% filter(university == uni & lang %in% langs)

  tokens_uni <- uni_tweets %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem() %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(
          stopwords("en"), stopwords("de"),
          stopwords("fr"), stopwords("it"), extended_stopwords
        ), selection = "remove"
    )
  uni_dfm <- dfm(tokens_uni)
  # Apply LDA
  uni_dfm <- dfm_subset(uni_dfm, ntoken(uni_dfm) > 0)
  tweet_lda <- LDA(uni_dfm, k = 5, control = list(seed = 123))
  # Tidy the LDA results
  tweet_lda_td <- tidy(tweet_lda)
  # Extract top terms per topic
  top_terms <- tweet_lda_td %>%
    group_by(topic) %>%
    top_n(8, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
  # Visualize top terms per topic
  p <- top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~topic, scales = "free") +
    scale_y_reordered() +
    labs(
      x = "Beta (Term Importance within Topic)",
      y = NULL,
      title = paste("Top Terms per Topic in Tweets from", uni, "(LDA)")
    )
  print(p)
  # Topic Model Summary: top 10 terms per topic
  cat("\nTopic Model Summary for", uni, ":\n")
  print(as.data.frame(terms(tweet_lda, 10)))
}
```
#### Conclusion:
The LDA analysis reveals distinct topics across all tweets, emphasizing academic activities, events, and institutional developments. Overall the topics over all universities seem to be quite similar. Which indicates that the tweets are similar in content and style. However, the LDA model shows that topics like academic and research activities, student activities, and new developments are relevant in the tweets. Also because we stemmed the words could be not that understandable for the algorithm, this may have an effect on the choosen topics.

### Style Analysis
The distribution of tweet lengths shows variation across universities. Most tweets are concise, aligning with Twitter's character limit, but the exact length distribution differs among institutions. It is interesting to see that much tweets have around 150 words and that the tweets from the universities are not that long. It is a typical sign that the tweets are not that long and this is a common thing in social media.
```{r}
tweets %>%
  mutate(tweet_length = nchar(full_text)) %>%
  ggplot(aes(x = tweet_length)) +
  geom_histogram() +
  labs(title = "Distribution of Tweet Lengths")
```

### Sentiment Analysis
Sentiment analysis was conducted to evaluate the emotional tone of the tweets. The analysis used the Syuzhet method to calculate sentiment scores for each tweet. Syuzhet was chosen because of its ability to capture the emotions within text. Which makes it effective for analyzing narrative structures. Additionally, the NRC sentiment dictionary was utilized for its comprehensive coverage of emotions across multiple languages, including German, Italian, and French. See Documentation: https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html#:~:text=Multilingual%20Sentiment%20Lexicons. Together, Syuzhet and NRC provide a powerful combination for performing detailed and multilingual sentiment analysis.

Overall Sentiment Trends:
- The sentiment scores vary over time and by university, showing fluctuations in the emotional tone of the tweets.
- Positive words commonly found in tweets include terms related to academic achievements, collaborations, and positive experiences.
- Negative words often relate to challenges, competitions, and issues faced by the universities.

```{r}
# Calculate Sentiment for Supported Languages Only
langs <- c("de", "fr", "it", "en")

tweets_filtered <- tweets %>%
  filter(lang %in% langs)

# Function to get sentiment based on language
get_multilang_sentiment <- function(text, lang) {
  if (lang == "de") {
    return(get_sentiment(text, method = "nrc", language = "german"))
  } else if (lang == "it") {
    return(get_sentiment(text, method = "nrc", language = "italian"))
  } else if (lang == "fr") {
    return(get_sentiment(text, method = "nrc", language = "french"))
  } else if (lang == "en") {
    return(get_sentiment(text, method = "syuzhet"))
  } else {
    return(NA) # Return NA for unsupported languages
  }
}
# Calculate Syuzhet Sentiment for each Tweet
tweets_filtered$sentiment <-
  mapply(
    get_multilang_sentiment,
    tweets_filtered$full_text, tweets_filtered$lang
  )

plot_data <- tweets_filtered %>%
  group_by(university, month) %>%
  summarize(mean_sentiment = mean(sentiment, na.rm = TRUE))

# Convert month to Date format
plot_data <- plot_data %>%
  mutate(month = as.Date(month, format = "%Y-%m-%d"))

# Plot Sentiment by all Universities
ggplot(plot_data, aes(
  x = month,
  y = mean_sentiment,
  color = university, group = university
)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mean Sentiment Over Time by University",
    y = "Mean Sentiment Score",
    x = "Month"
  ) +
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_minimal()

for (uni in unique(tweets_filtered$university)) {
  most_used_lang <- tweets %>%
    filter(university == uni) %>%
    count(lang) %>%
    slice_max(n = 1, order_by = n) %>%
    pull(lang)

  uni_tweets <- tweets_filtered %>%
    filter(university == uni & lang == most_used_lang)

  plot_data <- uni_tweets %>%
    group_by(month) %>%
    summarize(mean_sentiment = mean(sentiment, na.rm = TRUE))

  # Plot Syuzhet Sentiment Over Time (Per University)
  print(ggplot(plot_data, aes(x = month, y = mean_sentiment, group = 1)) +
    geom_line() +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(
      title = paste0("Mean Syuzhet Sentiment Over Time by - ", uni),
      y = "Mean Sentiment Score",
      x = "Month"
    ))

  # Tokenize and Preprocess Words
  uni_words <- uni_tweets %>%
    unnest_tokens(word, full_text_emojis) %>%
    filter(nchar(word) > 3) %>%
    filter(!str_detect(word, "\\d")) %>%
    filter(!str_detect(word, "https?://\\S+|www\\.\\S+|t\\.co|http|https"))

  # Remove stopwords after counting word frequency
  word_counts <- uni_words %>%
    count(word, sort = TRUE) %>%
    anti_join(get_stopwords(language = most_used_lang), by = "word")

  sentiment_words <- word_counts %>%
    mutate(sentiment = get_multilang_sentiment(word, most_used_lang))

  # Separate Positive and Negative Words
  positive_words <- sentiment_words %>%
    filter(sentiment >= 0) %>%
    arrange(desc(n)) %>%
    rename(freq = n)

  negative_words <- sentiment_words %>%
    filter(sentiment < 0) %>%
    arrange(desc(n)) %>%
    rename(freq = n)

  # Create and Display Word Clouds
  print(paste0("Positive words for: ", uni, " in ", most_used_lang))
  print(head(positive_words, 20))
  wordcloud(
    words = positive_words$word,
    freq = positive_words$freq,
    max.words = 200,
    random.order = FALSE,
    colors = brewer.pal(8, "Dark2")
  )

  print(paste0("Negative words for: ", uni, " in ", most_used_lang))
  print(head(negative_words, 20))
  wordcloud(
    words = negative_words$word,
    freq = negative_words$freq,
    max.words = 200,
    random.order = FALSE,
    colors = brewer.pal(8, "Dark2")
  )
}
```

### Conclusion:
The analysis indicates that Swiss Universities of Applied Sciences exhibit diverse tweeting patterns in terms of content, style, and emotions. Tweets often focus on academic achievements, projects, and institutional news, with varying emotional tones across different universities.

## Question 4: What specific advice can you give us as communication department of BFH based on your analysis? How can we integrate the analysis of tweets in our internal processes, can you think of any data products that would be of value for us?
```{r}
# Language Analysis
tweets %>%
  filter(university == "bfh") %>%
  count(lang) %>%
  arrange(desc(n))

# Pie chart of langugaes
tweets %>%
  filter(university == "bfh") %>%
  count(lang) %>%
  ggplot(aes(x = "", y = n, fill = lang)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y") +
  labs(title = "Language Distribution of Tweets by BFH")

# Emoji Analysis
emoji_count <- tweets %>%
  filter(university == "bfh") %>%
  top_n_emojis(full_text)

print(emoji_count)
# Sum of all emojis used
print(sum(emoji_count$n))

emoji_count %>%
  mutate(emoji_name = reorder(emoji_name, n)) %>%
  ggplot(aes(n, emoji_name)) +
  geom_col(fill = "#37556E") +
  labs(
    x = "Count",
    y = "Emoji",
    title = "Top 20 Emojis Used by BFH"
  ) +
  theme_minimal()

heatmap_data_bfh <- tweets %>%
  filter(university == "bfh") %>%
  count(day, timeofday_hour) %>%
  complete(day, timeofday_hour, fill = list(n = 0))

# Show the pattern of the post time by day and hour. You can see clearly the working hours which are the time where the most tweets are posted.
ggplot(heatmap_data_bfh, aes(x = day, y = timeofday_hour, fill = n)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Heatmap of Tweet Activity", x = "Day", y = "Hour") +
  theme_minimal()

engagement_hour_bfh <- tweets %>%
  filter(university == "bfh") %>%
  group_by(timeofday_hour) %>%
  summarise(avg_engagement = mean(weighted_engagement, na.rm = TRUE))

# When we look at the engagement by hour, we can see that the most engagement is around 4pm until 8pm. There is also a slight peak in the midday
ggplot(engagement_hour, aes(x = timeofday_hour, y = avg_engagement)) +
  geom_bar(stat = "identity", fill = "#37556E") +
  labs(title = "Average Engagement by Hour", x = "Hour", y = "Avg Engagement") +
  theme_minimal()

plot_data_bfh <- tweets_filtered %>%
  filter(university == "bfh") %>%
  group_by(month) %>%
  summarize(mean_sentiment = mean(sentiment, na.rm = TRUE))

ggplot(plot_data_bfh, aes(x = month, y = mean_sentiment)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mean Syuzhet Sentiment Over Time by BFH",
    y = "Mean Sentiment Score",
    x = "Month"
  ) +
  theme_minimal()

insights <- list(
  "Most Active Hours" = hours_with_most_tweets_by_uni,
  "Most Active Days" = days_with_most_tweets_by_uni,
  "Content Analysis" = head(words_freqs_de),
  "Sentiment Analysis" = head(tweets_filtered$sentiment)
)
```
### Key Insights
Based on the analysis, the following recommendations and key insights can be made to enhance BFH's communication strategy:

#### 1. Language and Emoji Usage:
- **Language**: BFH predominantly tweets in German, with 3008 tweets in this language.
- **Emojis**: There are emojis like "👉" (backhand index pointing right), "🔋" (battery), and "👇" (backhand index pointing down) that are frequently used. However, the overall emoji usage is relatively low.

#### 2. Optimal Posting Times and Days:
- **Most Active Hours**: The analysis revealed that 8 AM is the most active hour for BFH tweets. However, tweets posted around 4 PM and 8 PM tend to receive higher engagement.
- **Most Active Days**: Tuesday is the most active day for BFH tweets. This resonates already well with the most liked tweets. But the peak on Tuesday is not very significant.

#### 3. Content and Sentiment Analysis:
- **Content Themes**: Frequent words in BFH tweets include "Bern", "Projekt" (project), and "Hochschule" (university). This indicates a focus on academic projects and institutional updates.
- **Sentiment**: Positive words often relate to academic achievements and successful projects, while negative words relate to challenges and competitions. A challenge is often subtitued with problem or issue. We suggest to use more positive words in the tweets to get a better sentiment score.

#### 4. Engagement Patterns:
- **Peak Engagement**: The highest average engagement occurs around 8 PM, with notable peaks at 4 PM and 6 PM.
- **Consistent Engagement**: Engagement remains relatively consistent from 6 AM to 5 PM, with moderate fluctuations.

### Recommendations:

- **Twet Schedule**: Schedule tweets during the most active hours (4 PM, 6 PM, and 8 PM) to maximize engagement. Consider aligning important announcements and updates with these times and specific days for better visibility. Implement a regular schedule, for example, posting about new projects or student achievements each Tuesday at 4 PM.
- **Multilangual Tweets**: The BFH may enhance its multilingual communication strategy by incorporating tweets in English and French to reach a broader audience.
- **Emphasize Positive Themes**: Highlight student achievements, successful projects, and positive institutional news to generate positive reactions. Posts like "Congratulations to our students for their successful bachelor thesis" will likely receive positive reactions.
- **Utilize Topic Insights**: Emphasize key themes such as academic projects, student updates, and digital initiatives to align content with audience interests.
- **Emojis**: Conduct further research on the impact of emojis on tweet popularity, as current analysis does not show a strong correlation.
- **Interactive Content**: Use polls, Q&A sessions, and live tweets during events to increase engagement.
- **User-Generated Content**: Encourage students and faculty to share their experiences and tag the university in their posts. Retweeting and highlighting these posts can create a sense of community and increase engagement.
- **Demographic Insights**: Analyze the demographics of the followers engaging with BFH tweets. Tailor content to resonate with different audience segments. This is not covered in this document.

### Integrating Tweet Analysis into Internal Processes:

#### 1. Real-Time Tweet Dashboard:
Implement a dashboard to track tweet performance, including engagement metrics, sentiment scores, and trending topics. This allows adjusting the tweet schedule and audience targeting in real-time. Monthly reports could also be sufficient for strategic adjustments.

#### 2. Content Calendar:
Develop a content calendar that aligns tweet releases with peak engagement times and days. This helps plan tweets in advance and ensures they are posted at optimal times.

#### 3. Feedback Loop:
Establish a feedback loop where the communication team reviews analytics data and adjusts strategies according to the findings. Regular team meetings can help discuss insights and support a data-driven approach to communication.

### Potential Data Products:

1. **Engagement Prediction Tool**: Develop a tool that predicts the best times to tweet based on historical data. This optimizes tweet scheduling for maximum engagement and can also predict periodic peaks in engagement.
2. **Sentiment Analysis Bot**: Develop a tool to analyze the sentiment of drafts before posting, ensuring an appropriate and positive tone. This should aldough not be used as a content controll (We're not in China)
3. **Trend Tracker**: Create a feature that identifies emerging topics and trends in real-time. This allows the communication team to quickly adapt and incorporate relevant themes into their messaging.
4. **Influencer Tool**: Identify key influencers who engage with BFH's tweets. This can include industry leaders, alumni, or current students with significant followings.

By integrating these recommendations and tools, BFH can enhance its communication strategy, ensuring that its messages are timely, relevant, and engaging for its audience.
