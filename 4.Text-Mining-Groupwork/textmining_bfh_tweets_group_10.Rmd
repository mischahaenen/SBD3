---
title: "Text Mining Groupwork 2: Analysis of Twitter Activities by Swiss Universities of Applied Sciences"
author: "Fabio Burri, Mischa Haenen, Robin Galeazzi and Timon Galeazzi"
date: "2024-05-20"
output: html_document
---
`
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(textclean)
library(wordcloud2)
library(sentimentr)
library(lexicon)
library(topicmodels)
library(tidytext)
library(quanteda)
library(stopwords)
library(syuzhet)
library(textdata)
library(cld3)
library(tidyEmoji)
library(dplyr)
library(lubridate)
library(reshape2)
library(stopwords)
```
Load the tweets and check if they are loaded correctly. We also check the summary for a first interpretation. The summary(tweets) output reveals the following:

- Time Range: Tweets span from 2009 to 2023, with most posted between 2015 and 2020.
- Distribution: The data is unevenly distributed, with the median date in 2018.
- Engagement: Retweets and favorites range from 0 to 267 and 188, respectively, with most tweets having low engagement.
- Universities: The university column indicates tweets from multiple universities, allowing for comparison.

```{r}
# Set working directory
# getwd()
# setwd("./data/")

# Load data
load("../data/Tweets_all.rda")

# Check that tweets are loaded
head(tweets)
summary(tweets)
```
Start preprocessing the tweets, to calculate the intervalls some additional properties are needed. The preprocessing steps transform raw tweet data into a structured format suitable for analysis. This includes:

- Standardizing Date and Time: Ensures accurate temporal analysis.
- Extracting Features: Derives relevant information (year, month, day, language) for targeted analysis.
- Handling Emojis: Converts emojis to text for sentiment analysis and visualization.
- Data Cleaning: Removes unnecessary tags and standardizes data types.
```{r}
# Preprocessing Step: Convert date and time to POSIXct and format according to date, year and university. Detect language and extract emojis. The days are sorted from the system locale starting from monday
tweets <- tweets %>%
  mutate(
    created_at = as.POSIXct(created_at, format = "%Y-%m-%d %H:%M:%S"),
    date = as.Date(created_at),
    day = lubridate::wday(created_at,
      label = TRUE, abbr = FALSE,
      week_start = getOption("lubridate.week.start", 1),
      locale = Sys.getlocale("LC_TIME")
    ),
    year = year(created_at),
    month = floor_date(created_at, "month"),
    university = as.character(university),
    lang = detect_language(full_text),
    full_text_emojis = replace_emoji(full_text, emoji_dt = lexicon::hash_emojis)
  )

# Remove Emoji Tags helper funciton
# replace emoji places the emojis in the text as tags and their name, we remove them here
remove_emoji_tags <- function(text) {
  str_remove_all(text, "<[a-z0-9]{2}>")
}
# Remove Emoji Tags
tweets$full_text_emojis <- sapply(tweets$full_text_emojis, remove_emoji_tags)

# Store emojis in a sep arate column to analyze later
tweets$emoji_unicode <- tweets %>%
  emoji_extract_nest(full_text) %>%
  select(.emoji_unicode)
```

## Question 1: How many tweets are being posted by the various Universities when? Are there any 'release' strategies visible?

### Most Active Hours:
Each university has a distinct peak hour for tweeting, often aligning with typical working hours (9 AM - 5 PM). This suggests a strategic approach to reach their target audience when they are most likely online. The most active hours for each university are as follows:

- FHNW: 9 AM
- FH Graub√ºnden: 11 AM
- ZHAW: 5 PM
- BFH: 8 AM
- HES-SO: 10 AM
- HSLU: 9 AM
- OST-FH: 8 AM
- SUPSI-CH: 11 AM

These times typically align with standard working hours, indicating a strategic approach to reach their audience during times they are most likely to be online. It appears that a typical worker is more productive and active on Twitter in the morning, with motivation waning around midday and continuing to decline until the end of the workday.

### Most Active Days:
There isn't a consistent "most active day" across universities. Some favor weekdays, while others show higher activity on weekends. This could reflect differences in their target audience or the nature of their content.

- FHNW: Tuesday
- FH Graub√ºnden: Tuesday
- ZHAW: Wednesday
- BFH: Tuesday
- HES-SO: Tuesday
- HSLU: Thursday
- OST-FH: Friday
- SUPSI-CH: Friday

The pattern also suggests that tweet activity tends to be higher earlier in the week, with motivation and tweet frequency potentially falling as the week progresses.

### Release Strategies:
strategy and perhaps a more reactive approach to current events or trends.
While universities have peak hours and days, the intervals between tweets vary significantly, indicating a more reactive strategy rather than a rigid release schedule. This variability suggests that the universities might be responding to real-time events or trends rather than sticking to a strict posting schedule.

```{r}
# Count each tweet by university and hour of the day
tweet_counts_by_hour_of_day <- tweets %>%
  group_by(university, timeofday_hour) %>%
  count() %>%
  arrange(university, timeofday_hour)

# Plot the number of tweets by university and hour of the day
ggplot(
  tweet_counts_by_hour_of_day,
  aes(
    x = timeofday_hour, y = n,
    color = university, group = university
  )
) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and hour",
    x = "Hour of day",
    y = "Number of tweets"
  )

# Show most active hours for each university
hours_with_most_tweets_by_uni <- tweet_counts_by_hour_of_day %>%
  group_by(university, timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(hours_with_most_tweets_by_uni)

# Show most active hour overall
hour_with_most_tweets <- tweet_counts_by_hour_of_day %>%
  group_by(timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  arrange(desc(total_tweets)) %>%
  slice_max(n = 1, order_by = total_tweets)

print(hour_with_most_tweets)

# Count each tweet by university and weekday
tweet_counts_by_week_day <- tweets %>%
  group_by(university, day) %>%
  count() %>%
  arrange(university, day)

# Plot the number of tweets by university and day of the week
ggplot(
  tweet_counts_by_week_day,
  aes(
    x = day, y = n,
    color = university,
    group = university
  )
) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and day of the week",
    x = "Day of the week",
    y = "Number of tweets"
  )

# Show most active days for each university
days_with_most_tweets_by_uni <- tweet_counts_by_week_day %>%
  group_by(university, day) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(days_with_most_tweets_by_uni)

# Calculate time intervals between tweets
find_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

tweets <- tweets %>%
  arrange(university, created_at) %>%
  group_by(university) %>%
  mutate(time_interval = as.numeric(
    difftime(created_at, lag(created_at), units = "mins")
  ))

# Descriptive statistics of time intervals
summary(tweets$time_interval)

# setwd("../4.Text-Mining-Groupwork/plots")
unique_years <- tweets$year %>% unique()
# Pilot distribution of time intervals between tweets for each year
for (curr_year in unique_years) {
  # Filter data for the specific year
  filtered_data <- tweets %>%
    filter(year(created_at) == curr_year)

  print(ggplot(filtered_data, aes(x = time_interval)) +
    geom_histogram(fill = "lightblue") +
    facet_wrap(~university) +
    labs(
      title = paste0(
        "Distribution of time intervals between tweets - ", curr_year
      ),
      x = "Time interval (minutes)",
      y = "Tweet count"
    ))
  universities <- filtered_data$university %>% unique()
  for (uni in universities) {
    # Filter data for the specific university
    uni_filtered_data <- filtered_data %>%
      filter(university == uni)

    print(ggplot(uni_filtered_data, aes(x = time_interval)) +
      geom_histogram(fill = "lightblue") +
      labs(
        title = paste0(
          "Distribution of time intervals between tweets for ", uni,
          " in ", curr_year
        ),
        x = "Time interval (minutes)",
        y = "Tweet count"
      ))
    # Calculate mode (most common interval) in hours
    most_common_interval_minutes <- find_mode(uni_filtered_data$time_interval)
    most_common_interval_hours <- most_common_interval_minutes / 60
    print(paste0(
      "Most common time interval for ", uni,
      " in ",
      curr_year,
      " is ", most_common_interval_minutes,
      " minutes (", most_common_interval_hours, " hours)"
    ))
  }
}
```

### Overall Trends:
Most Common Time Interval for Tweets: The time intervals between tweets also vary widely among the universities, with some universities having very frequent posts (every few minutes) while others have longer intervals (several hours or even days).
Most Active Hour Overall: Across all universities, 11 AM is the most common hour for tweets, with a total of 2356 tweets posted at this time.
### Conclusion:
The data indicates that Swiss Universities of Applied Sciences primarily tweet during working hours and show distinct patterns in their most active days and hours. Workers tend to be more productive and active on Twitter in the morning, with a noticeable decline in activity around midday and towards the end of the week. This data-driven approach to analyzing Twitter activity can help universities optimize their social media strategies by identifying the best times and days to engage their audiences.

## Question 2: What are the tweets about and how do other Twitter users react to them (likes, etc.)?

### Data Preprocessing
```{r}
langs <- c("de", "fr", "it", "en")
tweets_filtered <- tweets %>%
  filter(lang %in% langs)
# Define extended stopwords (outside loop for efficiency)
# Remove 'amp' as it is not meaningful because its only & symbol
# Remove 'rt' because it is an word e.g 'engagiert'.
extended_stopwords <- c(
  "#fhnw", "#bfh", "@htw_chur", "#hslu", "#supsi", "#sups",
  "amp", "rt", "fr", "ber", "t.co", "https", "http", "www", "com", "html"
)
# Create separate DFMs for each language
dfm_list <- list()
for (sel_lang in langs) {
  # Subset tweets for the current language
  tweets_lang <- tweets_filtered %>%
    filter(lang == sel_lang)
  # Create tokens for the current language
  stopwords_lang <- stopwords(sel_lang)
  # Create tokens for all tweets:
  # - create corpus and tokens because tokensonly works on character, corpus, list, tokens, tokens_xptr objects.
  # - create tokens and remove: URLS, Punctuation, Numbers, Symbols, Separators
  # - transform to lowercase
  # - Stem all words
  # - Create n-grams of any length (not includinf bigrams and trigrams but they are shown later)
  # - It is important to remove the stopwords after stemming the words because we remove the endings from some stem words
  tokens_lang <- tweets_lang %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem(lang = sel_lang) %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(stopwords_lang, extended_stopwords), selection = "remove"
    )
  # Create DFM for the current language
  dfm_list[[sel_lang]] <- dfm(tokens_lang)
}
```
### Content Analysis
Tweets were analyzed across four languages: German, French, Italian, and English. Each university tends to tweet predominantly in one or more languages, reflecting the linguistic diversity of Switzerland.

- German: Predominantly used by BFH and FHNW. Common words include "neu" (new), "mehr" (more), "schweiz" (Switzerland), and "studier" (study).
- French: Primarily used by HES-SO. Common words include "projet" (project), "recherch" (research), and "tudi" (study).
- Italian: Mostly used by SUPSI. Common words include "nuov" (new), "progett" (project), and "student" (student).
- English: Frequently used by HSLU. Common words include "student", "project", "thank", and "university".
### Word Frequency:
- English: The most frequent words were "student", "new", "@hslu", "university", "project", "thank", "@zhaw", "day", "science", and "today".
- German: The most frequent words were "neu", "mehr", "schweiz", "werd", "all", "studier", "heut", "hochschul", "bfh", and "jahr".
- Italian: The most frequent words were "nuov", "sups", "progett", "student", "present", "info", "iscrizion", "cors", "ricerc", and "formazion".
- French: The most frequent words were "hes-so", "right", "arrow", "dan", "projet", "a", "tudi", "haut", "col", and "@hes_so".

It's important to note that some words like "right" üëâ and "arrow" ‚û°Ô∏è are actually names of parsed emojis and not written words in the tweets.

Word clouds for each language visually depicted the most common words, emphasizing their relative frequencies. The analysis revealed that universities tweet in multiple languages, reflecting the linguistic diversity of their audience. The most common words often related to educational themes, projects, and institutional news, indicating a focus on academic content.
```{r}
# Word Frequencies & Visualization
words_freqs_en <- sort(colSums(dfm_list$en), decreasing = TRUE)
head(words_freqs_en, 20)
wordcloud2(data.frame(
  word = names(words_freqs_en),
  freq = words_freqs_en
), size = 0.5)

words_freqs_de <- sort(colSums(dfm_list$de), decreasing = TRUE)
head(words_freqs_de, 20)
wordcloud2(data.frame(
  word = names(words_freqs_de),
  freq = words_freqs_de
), size = 0.5)

word_freqs_it <- sort(colSums(dfm_list$it), decreasing = TRUE)
head(word_freqs_it, 20)
wordcloud2(data.frame(
  word = names(word_freqs_it),
  freq = word_freqs_it
), size = 0.5)
# It seems that there are some english words but I think this are emojis
word_freqs_fr <- sort(colSums(dfm_list$fr), decreasing = TRUE)
head(word_freqs_fr, 20)
wordcloud2(data.frame(
  word = names(word_freqs_fr),
  freq = word_freqs_fr
), size = 0.5)
# University-specific Analysis
for (uni in unique(tweets$university)) {
  # Subset tweets for the current language
  uni_tweets <- tweets_filtered %>%
    filter(university == uni)

  tokens_lang <- uni_tweets %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem() %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(
          stopwords("en"), stopwords("de"),
          stopwords("fr"), stopwords("it"), extended_stopwords
        ), selection = "remove"
    )
  # Create Data Frame Matrix for uni with all languages
  uni_dfm <- dfm(tokens_lang)
  # Word Frequencies
  uni_word_freqs <- sort(colSums(uni_dfm), decreasing = TRUE)
  # print most common words: the emoji right are used often
  head(uni_word_freqs, 20)
  wordcloud2(data.frame(
    word = names(uni_word_freqs),
    freq = uni_word_freqs
  ), size = 0.5)
}
```
### Userreaction Analysis
A weighted engagement metric was calculated to measure user reactions, considering both likes (favorites) and retweets, with retweets given double weight.

Posting Times of Most Engaged Tweets: The analysis of the posting times of the most engaged tweets (top 1000 by engagement) showed that:

- The most engaged tweets were posted throughout the day, with a notable peak around mid-morning (11 AM).
- This pattern aligns with the overall finding that users tend to be more active and engaged during typical working hours. Which can be a sign, that the users are more active during the day and not during the night. Or the customers of the universities are working also in a university or are students.

### Content of Most Engaged Tweets:
The most common words in the most engaged tweets included "mehr" (more), "neue" (new), "schweiz" (Switzerland), "schweizer" (Swiss), "right", "heut" (today), "zeigt" (shows), "#hsluinformatik" (HSLU informatics), "studi" (study), and "zhaw". Again, "right" and similar terms are names of emojis and not actual 
```{r}
# Calculate a 'weighted engagement' metric
tweets <- tweets %>%
  mutate(
    weighted_engagement = favorite_count * 1 + retweet_count * 2
  )

# Identify tweets with the highest weighted engagement
most_engaged_tweets <- tweets %>%
  arrange(desc(weighted_engagement)) %>%
  head(1000) # Top 1000 for analysis

# Analyze posting time of most engaged tweets (same as before)
most_engaged_tweets_time <- most_engaged_tweets %>%
  mutate(time_of_day = format(created_at, "%H"))

ggplot(most_engaged_tweets_time, aes(x = as.numeric(time_of_day))) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "blue") +
  labs(
    title = "Distribution of Posting Times for Most Engaged Tweets",
    x = "Hour of Day",
    y = "Frequency"
  )
```
Analyse the content of the most liked tweets
```{r}
# Preprocessing content of most liked tweets
tokens_most_engaged <- most_engaged_tweets %>%
  corpus(text_field = "full_text_emojis") %>%
  tokens(
    remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
    remove_url = TRUE, remove_separators = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_wordstem(lang = sel_lang) %>%
  tokens_ngrams(n = 1) %>%
  tokens_select(
    pattern =
      c(
        stopwords("en"), stopwords("de"),
        stopwords("fr"), stopwords("it"), extended_stopwords
      ), selection = "remove"
  )
tokens_most_engaged_dfm <- dfm(tokens_most_engaged)
freqs_most_engaged <- sort(colSums(tokens_most_engaged_dfm), decreasing = TRUE)
# print most common words: the emoji right are used often
head(freqs_most_engaged, 20)
set.seed(123)
wordcloud2(data.frame(
  word = names(freqs_most_engaged),
  freq = freqs_most_engaged
), size = 0.5)
```

The analysis indicates that Swiss Universities of Applied Sciences tweet in multiple languages, reflecting the linguistic diversity of their audience. The tweets often focus on educational themes, projects, and institutional news. User engagement is highest for tweets posted during working hours, with the most engaging content often including timely updates and relevant academic information. Recognizing the role of emojis in enhancing engagement, universities can further optimize their social media strategies to maximize reach and impact.
## Question 3: How do the university tweets differ in terms of content, style, emotions, etc?

### Content Analysis (Word Clouds)
Each university shows distinct patterns in the words and emojis used in their tweets. The analysis involved creating word clouds and identifying the most common words and emojis.

Most Common Words:

- FHNW: Common words include "mehr" (more), "hochschul" (university), and "studierend" (students).
- FH Graub√ºnden: Words like "chur" (location), "htw" (university abbreviation), and "busi" (business) are frequent.
- ZHAW: Frequent words include "zhaw" (university abbreviation), "engineering", and "schweizer" (Swiss).
- BFH: Common terms are "bern" (location), "projekt" (project), and "hochschul" (university).
- HES-SO: Words like "hes-so" (university abbreviation), "projet" (project), and "tudiant" (student) are prevalent.
- HSLU: Common words are "hslu" (university abbreviation), "luzern" (location), and "hochschul" (university).
- OST-FH: Frequent terms include "ost" (university abbreviation), "st.gallen" (location), and "podcast".
- SUPSI-CH: Words like "supsi" (university abbreviation), "formazion" (education), and "progetto" (project) are prevalent.

Most Common Emojis:
- FHNW: Top emojis include üëâ (backhand index pointing right), üíõ (yellow heart), and üñ§ (black heart).
- FH Graub√ºnden: Frequent emojis are üéâ (party popper), üòÉ (grinning face with big eyes), and üòä (blush).
- ZHAW: Common emojis include üëâ (backhand index pointing right), ‚ö° (high voltage), and üòâ (wink).
- BFH: Top emojis are üëâ (backhand index pointing right), üîã (battery), and üëá (backhand index pointing down).
- HES-SO: Common emojis are üëâ (backhand index pointing right), üéì (graduation cap), and ‚û° (arrow right).
- HSLU: Top emojis include üéì (graduation cap), üë® (man), and üöÄ (rocket).
- OST-FH: Frequent emojis are üëâ (backhand index pointing right), ‚û° (arrow right), and üéì (graduation cap).
- SUPSI-CH: Common emojis include üëâ (backhand index pointing right), üéì (graduation cap), and üéâ (party popper).
```{r}
for (uni in unique(tweets$university)) {
  uni_tweets <- tweets %>%
    filter(university == uni, lang %in% langs)
  tokens_uni <- uni_tweets %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem() %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(
          stopwords("en"), stopwords("de"),
          stopwords("fr"), stopwords("it"), extended_stopwords
        ), selection = "remove"
    )
  uni_dfm <- dfm(tokens_uni)
  freqs_uni <- sort(colSums(uni_dfm), decreasing = TRUE)
  # print most common words: the emoji right are used often
  head(freqs_uni, 20)
  set.seed(123)
  wordcloud2(data.frame(
    word = names(freqs_uni),
    freq = freqs_uni
  ), size = 0.5)

  # Analyze Top Emojis by University
  emoji_count_per_university <- uni_tweets %>%
    top_n_emojis(full_text)

  print(emoji_count_per_university)

  emoji_count_per_university %>%
    mutate(emoji_name = reorder(emoji_name, n)) %>%
    ggplot(aes(n, emoji_name)) +
    geom_col() +
    labs(x = "Count", y = NULL, title = "Top 20 Emojis Used")
}
# Generate general tokens for bigram and trigram analysis
tokens <- tweets %>%
  corpus(text_field = "full_text_emojis") %>%
  tokens(
    remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
    remove_url = TRUE, remove_separators = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_wordstem() %>%
  tokens_select(
    pattern =
      c(
        stopwords("en"), stopwords("de"),
        stopwords("fr"), stopwords("it"), extended_stopwords
      ), selection = "remove"
  )
# Bigram Wordcloud
bi_gram_tokens <- tokens_ngrams(tokens, n = 2)
dfm_bi_gram <- dfm(bi_gram_tokens)
freqs_bi_gram <- sort(colSums(dfm_bi_gram), decreasing = TRUE)
head(freqs_bi_gram, 20)
# Create the bigram word cloud
set.seed(123)
wordcloud2(data.frame(
  word = names(freqs_bi_gram),
  freq = freqs_bi_gram
), size = 0.5)

# Trigram Wordcloud
tri_gram_tokens <- tokens_ngrams(tokens, n = 3)
dfm_tri_gram <- dfm(tri_gram_tokens)
reqs_tri_gram <- sort(colSums(dfm_tri_gram), decreasing = TRUE)
head(reqs_tri_gram, 20)
# Create the bigram word cloud
set.seed(123)
wordcloud2(data.frame(
  word = names(reqs_tri_gram),
  freq = reqs_tri_gram
), size = 0.5)
```
### LDA Topic Modeling
```{r}
# Source: Christoph Zangger -> l√∂scht alle Reihen mit nur 0s
new_dfm <- dfm_subset(dfm_list$en, ntoken(dfm_list$en) > 0)
tweet_lda <- LDA(new_dfm, k = 5, control = list(seed = 123))
# Tidy the LDA results
topic_terms <- tidy(tweet_lda, matrix = "beta")
# Extract topics and top terms
topics <- as.data.frame(terms(tweet_lda, 50)) # First fifty words per topic

# Extract top terms per topic
top_terms <- topic_terms %>%
  group_by(topic) %>%
  top_n(8, beta) %>% # Show top 8 terms per topic
  ungroup() %>%
  arrange(topic, -beta)

# Visualize top terms per topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered() +
  labs(
    x = "Beta (Term Importance within Topic)",
    y = NULL,
    title = "Top Terms per Topic in Tweets (LDA)"
  )

# Most different words among topics (using log ratios)
diff <- topic_terms %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001) %>%
  mutate(
    logratio_t1t2 = log2(topic2 / topic1),
    logratio_t1t3 = log2(topic3 / topic1),
    logratio_t2t3 = log2(topic3 / topic2)
  )
diff

# LDA Topic Modeling for each university
universities <- unique(tweets$university)

for (uni in universities) {
  # Filter tweets for the current university
  uni_tweets <- tweets %>% filter(university == uni)

  tokens_uni <- uni_tweets %>%
    corpus(text_field = "full_text_emojis") %>%
    tokens(
      remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
      remove_url = TRUE, remove_separators = TRUE
    ) %>%
    tokens_tolower() %>%
    tokens_wordstem() %>%
    tokens_ngrams(n = 1) %>%
    tokens_select(
      pattern =
        c(
          stopwords("en"), stopwords("de"),
          stopwords("fr"), stopwords("it"), extended_stopwords
        ), selection = "remove"
    )
  uni_dfm <- dfm(tokens_uni)
  # Apply LDA
  uni_dfm <- dfm_subset(uni_dfm, ntoken(uni_dfm) > 0)
  tweet_lda <- LDA(uni_dfm, k = 5, control = list(seed = 123))
  # Tidy the LDA results
  tweet_lda_td <- tidy(tweet_lda)
  # Extract top terms per topic
  top_terms <- tweet_lda_td %>%
    group_by(topic) %>%
    top_n(8, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
  # Visualize top terms per topic
  p <- top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~topic, scales = "free") +
    scale_y_reordered() +
    labs(
      x = "Beta (Term Importance within Topic)",
      y = NULL,
      title = paste("Top Terms per Topic in Tweets from", uni, "(LDA)")
    )
  print(p)
  # Topic Model Summary: top 10 terms per topic
  cat("\nTopic Model Summary for", uni, ":\n")
  print(as.data.frame(terms(tweet_lda, 10)))
}
```
### Style Analysis
The distribution of tweet lengths shows variation across universities. Most tweets are concise, aligning with Twitter's character limit, but the exact length distribution differs among institutions. It is interesting to see that much tweets have around 150 words and that the tweets from the universities are not that long. It is a typical sign that the tweets are not that long and this is a common thing in social media.
```{r}
tweets %>%
  mutate(tweet_length = nchar(full_text)) %>%
  ggplot(aes(x = tweet_length)) +
  geom_histogram() +
  labs(title = "Distribution of Tweet Lengths")
```
### Sentiment Analysis
Sentiment analysis was conducted to evaluate the emotional tone of the tweets. The analysis used the Syuzhet method to calculate sentiment scores for each tweet.

Overall Sentiment Trends:
- The sentiment scores vary over time and by university, showing fluctuations in the emotional tone of the tweets.
- Positive words commonly found in tweets include terms related to academic achievements, collaborations, and positive experiences.
- Negative words often relate to challenges, competitions, and issues faced by the universities.

Sentiment by University:
- FHNW: Positive words include "academy", "accelerate", and "activities". Negative words include "avoid", "bacteria", and "challenge".
- FH Graub√ºnden: Positive words include "able", "academic", and "advantage". Negative words include "competition", "corruption", and "fire".
- ZHAW: Positive words include "abilities", "academic", and "achievement". Negative words include "barrier", "challenge", and "competition".
- BFH: Positive words include "academic", "access", and "activities". Negative words include "aggression", "competition", and "fail".
- HES-SO: Positive words include "academic", "active", and "amazing". Negative words include "confessions", "failure", and "hard".
- HSLU: Positive words include "academic", "access", and "achievement". Negative words include "addiction", "challenge", and "fail".
- OST-FH: Positive words include "announce", "beautiful", and "collaboration". Negative words are minimal, including "dire" and "fire".
- SUPSI-CH: Positive words include "academic", "access", and "achievement". Negative words include "barrier", "cloud", and "danger".
```{r}
# Calculate Sentiment for Supported Languages Only
langs <- c("de", "fr", "it", "en")

tweets_filtered <- tweets %>%
  filter(lang %in% langs)

# TODO: because sentiment only works for english
# Create Function to Get Syuzhet Sentiment
get_syuzhet_sentiment <- function(text, lang) {
  # Check if language is supported
  if (lang %in% langs) {
    return(get_sentiment(text, method = "syuzhet", lang = lang))
  } else {
    return(NA) # Return NA for unsupported languages
  }
}

# Calculate Syuzhet Sentiment for each Tweet
tweets_filtered$sentiment <-
  mapply(get_syuzhet_sentiment, tweets_filtered$full_text, tweets_filtered$lang)

plot_data <- tweets_filtered %>%
  group_by(university, month) %>%
  summarize(mean_sentiment_syuzhet = mean(sentiment, na.rm = TRUE))

# Plot Syuzhet Sentiment by all Universities
ggplot(plot_data, aes(
  x = month,
  y = mean_sentiment_syuzhet,
  color = university, group = university
)) +
  geom_line() +
  labs(
    title = "Mean Syuzhet Sentiment Over Time by University",
    y = "Mean Sentiment Score"
  ) +
  scale_x_datetime(date_breaks = "1 month", date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

for (uni in unique(tweets$university)) {
  uni_tweets <- tweets %>%
    filter(university == uni, lang == "en")

  uni_tweets$sentiment <-
    mapply(get_syuzhet_sentiment, uni_tweets$full_text, uni_tweets$lang)

  plot_data <- uni_tweets %>%
    group_by(month) %>%
    summarize(mean_sentiment = mean(sentiment, na.rm = TRUE))

  # Plot Syuzhet Sentiment Over Time (Per University)
  print(ggplot(plot_data, aes(x = month, y = mean_sentiment, group = 1)) +
    geom_line() +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(
      title = paste0("Mean Syuzhet Sentiment Over Time by - ", uni),
      y = "Mean Sentiment Score",
      x = "Month"
    ))

  # Did not found a way to get the sentiment from the tweets for each language so I will use the full_text_emojis column and detect the language of the words only in german
  # Tokenize and Preprocess Words
  uni_words_en <- uni_tweets %>%
    unnest_tokens(word, full_text_emojis) %>%
    anti_join(get_stopwords(language = "en"), by = "word") %>%
    distinct() %>%
    filter(nchar(word) > 3) %>%
    filter(!str_detect(word, "\\d")) %>%
    filter(!str_detect(word, "https?://\\S+|www\\.\\S+|t\\.co|http|https"))


  sentiment_words_en <- uni_words_en %>%
    mutate(
      sentiment = get_sentiment(word, method = "syuzhet")
    )

  # Separate Positive and Negative Words
  positive_words_en <- sentiment_words_en %>%
    filter(sentiment >= 0) %>%
    count(word, sort = TRUE) %>%
    rename(freq = n)

  negative_words_en <- sentiment_words_en %>%
    filter(sentiment < 0) %>%
    count(word, sort = TRUE) %>%
    rename(freq = n)

  # Create and Display Word Clouds
  # positive words
  print(paste0("Positive words for: ", uni))
  print(head(positive_words_en, 20))
  print(wordcloud2(data.frame(
    word = positive_words_en$word,
    freq = positive_words_en$freq
  ), size = 0.5))

  print(paste0("Negative words for: ", uni))
  print(head(negative_words_en, 20))
  # negative words
  print(wordcloud2(data.frame(
    word = negative_words_en$word,
    freq = negative_words_en$freq
  ), size = 0.5))
}
```
### Conclusion:
The analysis indicates that Swiss Universities of Applied Sciences exhibit diverse tweeting patterns in terms of content, style, and emotions. Tweets often focus on academic achievements, projects, and institutional news, with varying emotional tones across different universities. Recognizing these patterns can help universities optimize their social media strategies to better engage with their audiences.
## Question 4: What specific advice can you give us as communication department of BFH based on your analysis? How can we integrate the analysis of tweets in our internal processes, can you think of any data products that would be of value for us?

The comprehensive analysis of BFH's tweets reveals several insights that can be leveraged to enhance the communication strategy.

### Language Analysis:
BFH predominantly tweets in German, with 2760 tweets in this language. This aligns with the linguistic preferences of their primary audience.

### Emoji Analysis:
The analysis of emoji usage shows that certain emojis are frequently used, which can be leveraged to increase engagement. Popular emojis like üéì (graduation cap) and üöÄ (rocket) often signify academic achievements and dynamic growth, resonating well with the audience.


### Summary key insights from the analysis
```{r}
# Language Analysis
tweets %>%
  filter(university == "bfh") %>%
  count(lang) %>%
  arrange(desc(n))

# Emoji Analysis
emoji_count <- tweets %>%
  top_n_emojis(full_text)

emoji_count %>%
  mutate(emoji_name = reorder(emoji_name, n)) %>%
  ggplot(aes(n, emoji_name)) +
  geom_col() +
  labs(x = "Count", y = NULL, title = "Top 20 Emojis Used")

insights <- list(
  "Most Active Hours" = hours_with_most_tweets_by_uni,
  "Most Active Days" = days_with_most_tweets_by_uni,
  "Content Analysis" = head(words_freqs_de),
  "Sentiment Analysis" = head(tweets_filtered$sentiment)
)
```

### Recommendations:
Based on the analysis, the following recommendations can be made to enhance BFH's communication strategy:
1. Optimize Tweet Release Times:
Based on the analysis of tweet activity, the most active hours for BFH are typically in the morning. Focusing on releasing tweets during these peak hours can maximize engagement. Scheduling important announcements and updates during these times will likely yield better visibility and interaction.
2. Focus on Specific Days for Announcements:
The analysis shows that Tuesday is the most active day for BFH tweets. Leveraging this day for critical updates and major announcements can ensure they reach a wider audience. Aligning content release schedules with these high-activity days can enhance communication effectiveness.
 Sentiment Analysis:
Sentiment analysis indicates the emotional tone of the tweets, helping tailor content to resonate positively with the audience. By understanding which types of tweets generate positive reactions, the communication team can craft messages that are more likely to be well-received. This could involve highlighting student achievements, successful projects, and positive institutional news.
4. Implement Topic Modeling:
Topic modeling reveals the key themes prevalent in the tweets. For BFH, topics often include academic projects, student updates, and digital initiatives. Aligning the communication strategy to emphasize these themes can enhance relevance and engagement. Regularly updating the communication team on trending topics can help keep the content aligned with audience interests.

### Integrating Tweet Analysis into Internal Processes:
To fully leverage these insights, the BFH communication department can integrate tweet analysis into their regular workflow:
1. Real-Time Analytics Dashboard: Implement a dashboard that tracks tweet performance, including engagement metrics, sentiment scores, and topic trends. This allows for real-time adjustments to the communication strategy.
2. Scheduled Reports: Generate weekly or monthly reports summarizing key metrics and insights. This helps the team stay informed about what content is performing well and where improvements can be made.
3. Content Calendar: Develop a content calendar that aligns tweet releases with peak engagement times and days. Incorporate findings from sentiment and topic analyses to plan content that resonates with the audience.
4. Feedback Loop: Establish a feedback loop where the communication team reviews analytics data and adjusts the strategy accordingly. Regular team meetings to discuss these insights can foster a more data-driven approach to communication.

### Potential Data Products:
To further enhance the communication strategy, BFH can consider developing data products that provide additional value:
1. Engagement Prediction Tool: A tool that predicts the best times to tweet based on historical data, optimizing tweet scheduling for maximum engagement.
2. Sentiment Analysis Bot: An automated system that analyzes the sentiment of drafts before they are posted, ensuring that the tone is appropriate and likely to generate positive reactions.
3. Trend Tracker: A feature that identifies emerging topics and trends in real-time, allowing the communication team to quickly adapt and incorporate relevant themes into their messaging.

By integrating these recommendations and tools, BFH can enhance its communication strategy, ensuring that its messages are timely, relevant, and engaging for its audience.
