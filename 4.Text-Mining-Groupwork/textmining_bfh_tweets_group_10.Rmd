---
title: "Text Mining Groupwork 2: Analysis of Twitter Activities by Swiss Universities of Applied Sciences"
author: "Fabio Burri, Mischa Haenen, Robin Galeazzi and Timon Galeazzi"
date: "2024-05-20"
output: html_document
---
`
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(textclean)
library(wordcloud)
library(sentimentr)
library(lexicon)
library(topicmodels)
library(tidytext)
library(quanteda)
library(stopwords)
library(syuzhet)
library(textdata)
library(cld3)
library(tidyEmoji)
```
Load the tweets and check if they are loaded correctly
```{r}
# Set working directory
getwd()
setwd("../data/")

# Load data
load("Tweets_all.rda")

# Check that tweets are loaded
head(tweets)
summary(tweets)
```
Start preprocessing the tweets, to calculate the intervalls some additional properties are needed.
```{r}
# Preprocessing Step: Convert date and time to POSIXct and format according to date, year and university

tweets <- tweets %>%
  mutate(
    created_at = as.POSIXct(created_at, format = "%Y-%m-%d %H:%M:%S"),
    date = as.Date(created_at),
    day = weekdays(created_at),
    day = factor(day, levels = c(
      "Monday", "Tuesday",
      "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"
    )),
    year = year(created_at),
    university = as.character(university),
    language = detect_language(full_text)
  )

tweets$emoji_unicode <- tweets %>%
  emoji_extract_nest(full_text) %>%
  select(.emoji_unicode)

# TODO: store name as well
```

## Question 1: How many tweets are being posted by the various Universities when? Are there any 'release' strategies visible?
```{r}
# Count each tweet by university and hour of the day
tweet_counts_by_hour_of_day <- tweets %>%
  group_by(university, timeofday_hour) %>%
  count() %>%
  arrange(university, timeofday_hour)

# Plot the number of tweets by university and hour of the day
ggplot(
  tweet_counts_by_hour_of_day,
  aes(
    x = timeofday_hour, y = n,
    color = university, group = university
  )
) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and hour",
    x = "Hour of day",
    y = "Number of tweets"
  )

# Show most active hours for each university
hours_with_most_tweets_by_uni <- tweet_counts_by_hour_of_day %>%
  group_by(university, timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(hours_with_most_tweets_by_uni)

# Show most active hour overall
hour_with_most_tweets <- tweet_counts_by_hour_of_day %>%
  group_by(timeofday_hour) %>%
  summarize(total_tweets = sum(n)) %>%
  arrange(desc(total_tweets)) %>%
  slice(1)

print(hour_with_most_tweets)

# Count each tweet by university and weekday
tweet_counts_by_week_day <- tweets %>%
  group_by(university, day) %>%
  count() %>%
  arrange(university, day)

# Plot the number of tweets by university and day of the week
ggplot(tweet_counts_by_week_day, aes(
  x = day,
  y = n, color = university,
  group = university
)) +
  geom_line() +
  facet_wrap(~university) +
  labs(
    title = "Number of tweets by university and day of the week",
    x = "Day of the week", y = "Number of tweets"
  )

# Show most active days for each university
days_with_most_tweets_by_uni <- tweet_counts_by_week_day %>%
  group_by(university, day) %>%
  summarize(total_tweets = sum(n)) %>%
  group_by(university) %>%
  slice_max(n = 1, order_by = total_tweets)

print(days_with_most_tweets_by_uni)

# Calculate time intervals between tweets
find_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

tweets <- tweets %>%
  arrange(university, created_at) %>%
  group_by(university) %>%
  mutate(time_interval = as.numeric(
    difftime(created_at, lag(created_at), units = "mins")
  ))

# Descriptive statistics of time intervals
summary(tweets$time_interval)

# setwd("../4.Text-Mining-Groupwork/plots")
unique_years <- tweets$year %>% unique()
# Plot distribution of time intervals between tweets for each year
for (curr_year in unique_years) {
  # Filter data for the specific year
  filtered_data <- tweets %>%
    filter(year(created_at) == curr_year)

  p <- ggplot(filtered_data, aes(x = time_interval)) +
    geom_histogram(fill = "lightblue") +
    facet_wrap(~university) +
    labs(
      title = paste0(
        "Distribution of time intervals between tweets - ", curr_year
      ),
      x = "Time interval (minutes)",
      y = "Tweet count"
    )
  print(p)
  universities <- filtered_data$university %>% unique()
  for (uni in universities) {
    # Filter data for the specific university
    uni_filtered_data <- filtered_data %>%
      filter(university == uni)
    # Calculate mode (most common interval) in hours
    most_common_interval_minutes <- find_mode(uni_filtered_data$time_interval)
    most_common_interval_hours <- most_common_interval_minutes / 60
    print(paste0(
      "Most common time interval for ", uni,
      " in ",
      curr_year,
      " is ", most_common_interval_minutes,
      " minutes (", most_common_interval_hours, " hours)"
    ))
  }
}

# TODO: Save plots by uni and year
```

## 2. What are the tweets about and how do other Twitter users react to them (likes, etc.)?

### Load the data and make previous changes undone
```{r}
getwd()
# setwd("../../data/")
# load("Tweets_all.rda")
```
### Data Preprocessing
```{r}
# Convert Emojis to Text
# TODO: Use for wordcloud?
tweets$full_text_with_emojis <- tweets$full_text %>%
  map_chr(~ replace_emoji(.x))
# Removes: URLS, Emojis, Punctuation, Numbers
tokens <- tokens(tweets$full_text,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE
)
# Remove 'amp' as it is not meaningful because its only & symbol
# Remove 'rt' because it is an word e.g 'engagiert'.
# TODO: Check if languages are correct
extended_stopwords <- c(
  stopwords("en"),
  stopwords("fr"),
  stopwords("de"),
  stopwords("it"),
  "#fhnw", "#bfh", "@htw_chur", "#hslu", "#supsi",
  "amp", "rt"
)
# transform to lowercase
tokens <- tokens_tolower(tokens)
# Stem all words
tokens <- tokens_wordstem(tokens)
# Create n-grams of any length (including bigrams and trigrams)
tokens <- tokens_ngrams(tokens, n = 1)
# remove stopwords im multiple languages and remove university hashtags
tokens <- quanteda::tokens_select(tokens,
  pattern = extended_stopwords,
  selection = "remove"
)
# Create Document-feature-matrix
doc_matrix <- dfm(tokens)
```
### Content Analysis
```{r}
# Word Frequencies
word_freqs <- doc_matrix %>%
  colSums() %>%
  sort(decreasing = TRUE)

# Top 20 words
head(word_freqs, 20)

word_freqs_df <- data.frame(
  word = featnames(doc_matrix),
  freq = colSums(doc_matrix)
)

# Create the word cloud
set.seed(123)
wordcloud(
  words = word_freqs_df$word,
  freq = word_freqs_df$freq,
  min.freq = 5,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.35,
  colors = brewer.pal(8, "Dark2")
)

# TODO: Wordcloud per University
```
### Userreaction Analysis
```{r}
# TODO: Add retweet and other stuff
# Identify the tweets with the most likes
most_liked_tweets <- tweets %>%
  arrange(desc(favorite_count)) %>%
  head(1000)

# Analyze the posting time of the most liked tweets
most_liked_tweets_time <- most_liked_tweets %>%
  mutate(time_of_day = format(created_at, "%H"))

# Plot the distribution of the posting times
ggplot(most_liked_tweets_time, aes(x = as.numeric(time_of_day))) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "blue") +
  labs(
    title = "Distribution of Posting Times for Most Liked Tweets",
    x = "Hour of Day",
    y = "Frequency"
  )
```
Analyse the content of the most liked tweets
```{r}
# Preprocessing content of most liked tweets
most_liked_tokens <- tokens(most_liked_tweets$full_text,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE
)

# Apply the same stopword removal and transformations

most_liked_tokens <- tokens_tolower(most_liked_tokens)
most_liked_tokens <- tokens_wordstem(most_liked_tokens)
most_liked_tokens <- tokens_ngrams(most_liked_tokens, n = 1:3)
most_liked_tokens <- tokens_select(most_liked_tokens,
  pattern = extended_stopwords, selection = "remove"
)
most_liked_doc_matrix <- dfm(most_liked_tokens)

# Word Frequencies of Most Liked Tweets
most_liked_word_freqs <- most_liked_doc_matrix %>%
  colSums() %>%
  sort(decreasing = TRUE)

# Create a word cloud for most liked tweets
most_liked_word_freqs_df <- data.frame(
  word = featnames(most_liked_doc_matrix),
  freq = colSums(most_liked_doc_matrix)
)

set.seed(123)
wordcloud(
  words = most_liked_word_freqs_df$word,
  freq = most_liked_word_freqs_df$freq,
  min.freq = 2,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.35,
  colors = brewer.pal(8, "Dark2")
)
# TODO: Per university
```
## Question 3: How do the university tweets differ in terms of content, style, emotions, etc?

### Content Analysis (Word Clouds)
```{r}
for (uni in unique(tweets$university)) {
  # Filter tweets by university
  uni_tweets <- tweets %>%
    filter(university == uni)

  uni_tokens <- tokens(uni_tweets$full_text,
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE,
    remove_separators = TRUE
  )
  # transform to lowercase
  uni_tokens <- tokens_tolower(uni_tokens)
  # Stem all words
  uni_tokens <- tokens_wordstem(uni_tokens)
  # Create n-grams of any length (including bigrams and trigrams)
  uni_tokens <- tokens_ngrams(uni_tokens, n = 1)

  uni_tokens <- tokens_select(uni_tokens,
    pattern = extended_stopwords, selection = "remove"
  )
  uni_dfm <- dfm(uni_tokens)

  uni_word_freqs_df <- data.frame(
    word = featnames(uni_dfm),
    freq = colSums(uni_dfm)
  )

  # Create the word cloud
  set.seed(123)
  wordcloud(
    words = uni_word_freqs_df$word,
    freq = uni_word_freqs_df$freq,
    min.freq = 5,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(8, "Dark2")
  )
  # TODO: Save wordcloud

  # Analyze Top Emojis by University
  emoji_count_per_university <- uni_tweets %>%
    top_n_emojis(full_text)

  emoji_count_per_university %>%
    mutate(emoji_name = reorder(emoji_name, n)) %>%
    ggplot(aes(n, emoji_name)) +
    geom_col() +
    labs(x = "Count", y = NULL, title = "Top 20 Emojis Used")
}

bi_gram_tokens <- tokens_ngrams(tokens, n = 2)
bi_gram_matrix <- dfm(bi_gram_tokens)

bi_gram_freqs_df <- data.frame(
  word = featnames(bi_gram_matrix),
  freq = colSums(bi_gram_matrix)
)

# Create the bigram word cloud
set.seed(123) # For reproducibility
wordcloud(
  words = bi_gram_freqs_df$word,
  freq = bi_gram_freqs_df$freq,
  min.freq = 3,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.35,
  colors = brewer.pal(8, "Accent")
)

# Trigram Wordcloud
tri_gram_tokens <- tokens_ngrams(tokens, n = 3)
tri_gram_matrix <- dfm(tri_gram_tokens)

tri_gram_freqs_df <- data.frame(
  word = featnames(tri_gram_matrix),
  freq = colSums(tri_gram_matrix)
)

# Create the trigram word cloud
set.seed(123) # For reproducibility
wordcloud(
  words = tri_gram_freqs_df$word,
  freq = tri_gram_freqs_df$freq,
  min.freq = 2,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0,
  colors = brewer.pal(8, "Paired")
)
```
### LDA Topic Modeling
```{r}
# TODO: Check why doc_matrix takes forever and doc_matrix does not work
tweet_lda <- LDA(word_freqs, k = 5, control = list(seed = 123))
# Tidy the LDA results
topic_terms <- tidy(tweet_lda, matrix = "beta")
# Extract topics and top terms
topics <- as.data.frame(terms(tweet_lda, 50)) # First fifty words per topic

# Tidy the LDA results
tweet_lda_td <- tidy(tweet_lda)

# Extract top terms per topic
top_terms <- tweet_lda_td %>%
  group_by(topic) %>%
  top_n(8, beta) %>% # Show top 8 terms per topic
  ungroup() %>%
  arrange(topic, -beta)

# Visualize top terms per topic
# TODO: not working until end of LDA
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered() +
  labs(
    x = "Beta (Term Importance within Topic)",
    y = NULL,
    title = "Top Terms per Topic in Tweets (LDA)"
  )

# Most different words among topics (using log ratios)
diff <- tweet_lda_td %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001) %>%
  mutate(
    logratio_t1t2 = log2(topic2 / topic1),
    logratio_t1t3 = log2(topic3 / topic1),
    logratio_t2t3 = log2(topic3 / topic2)
  )
diff

# Add topic probabilities to original data
lda_gamma <- tidy(tweet_lda, matrix = "gamma")
tweets <- tweets %>%
  mutate(document_id = row_number()) %>% # Add a unique ID for each tweet
  left_join(lda_gamma, by = c("document_id" = "document"))

# Analyze topics by university
tweets %>%
  count(university, topic, wt = gamma) %>%
  group_by(university) %>%
  slice_max(n = 3, order_by = n, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic)) %>%
  ggplot(aes(x = university, y = n, fill = topic)) +
  geom_col(position = "dodge") +
  labs(title = "Top 3 Topics by University", y = "Topic Proportion")
```
### Style Analysis
```{r}
tweets %>%
  mutate(tweet_length = nchar(full_text)) %>%
  ggplot(aes(x = tweet_length)) +
  geom_histogram() +
  labs(title = "Distribution of Tweet Lengths")
```
### Sentiment Analysis
```{r}
# Calculate Sentiment for Supported Languages Only
langs <- c("de", "fr", "it", "en")

tweets_filtered <- tweets %>%
  filter(language %in% langs)

# Create Function to Get Syuzhet Sentiment
get_syuzhet_sentiment <- function(text, lang) {
  # Check if language is supported
  if (lang %in% langs) {
    return(get_sentiment(text, method = "syuzhet", lang = lang))
  } else {
    return(NA) # Return NA for unsupported languages
  }
}

# Calculate Syuzhet Sentiment for each Tweet
tweets_filtered$sentiment <-
  mapply(get_syuzhet_sentiment, tweets_filtered$full_text, tweets_filtered$lang)

# Sentiment over Time
tweets_filtered$month <- floor_date(tweets_filtered$created_at, "month")

plot_data <- tweets_filtered %>%
  group_by(university, month) %>%
  summarize(mean_sentiment_syuzhet = mean(sentiment, na.rm = TRUE))

# Plot Syuzhet Sentiment by all Universities
ggplot(plot_data, aes(
  x = month,
  y = mean_sentiment_syuzhet,
  color = university, group = university
)) +
  geom_line() +
  labs(
    title = "Mean Syuzhet Sentiment Over Time by University",
    y = "Mean Sentiment Score"
  ) +
  scale_x_datetime(date_breaks = "1 month", date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# TODO: Check sentiment and use syuzhet
for (uni in unique(tweets$university)) {
  uni_tweets <- tweets %>%
    filter(university == uni, lang %in% langs)

  uni_tweets$sentiment <-
    mapply(get_syuzhet_sentiment, uni_tweets$full_text, uni_tweets$lang)

  # Sentiment Over Time (Per University)
  uni_tweets$month <- floor_date(uni_tweets$created_at, "month")
  uni_tweets$year <- year(uni_tweets$month)

  plot_data <- uni_tweets %>%
    group_by(year, month) %>%
    summarize(mean_sentiment = mean(sentiment, na.rm = TRUE))

  # Plot Syuzhet Sentiment Over Time (Per University)
  print(ggplot(plot_data, aes(x = month, y = mean_sentiment)) +
    geom_line(aes(color = as.factor(year))) +
    labs(
      title = paste0("Mean Syuzhet Sentiment Over Time by - ", uni),
      y = "Mean Sentiment Score"
    ) +
    scale_x_datetime(date_breaks = "1 month", date_labels = "%Y-%m") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    facet_wrap(~year, scales = "free_x"))

  uni_tweets <- tweets %>%
    filter(university == uni, language %in% langs)

  # Tokenize and Preprocess Words
  uni_words <- uni_tweets %>%
    unnest_tokens(word, full_text) %>%
    anti_join(stop_words) # remove Stopwords

  # Join Sentiment with Words
  sentiment_words <- uni_words %>%
    inner_join(get_sentiments("bing"), by = "word")

  # Separate Positive and Negative Words
  positive_words <- sentiment_words %>%
    filter(sentiment == "positive") %>%
    count(word, sort = TRUE)

  negative_words <- sentiment_words %>%
    filter(sentiment == "negative") %>%
    count(word, sort = TRUE)

  # Create and Display Word Clouds
  par(mfrow = c(1, 2)) # Set up side-by-side plots

  wordcloud(
    words = positive_words$word, freq = positive_words$n,
    scale = c(3, 0.5), max.words = 100, random.order = FALSE,
    rot.per = 0.35, colors = brewer.pal(8, "Greens")
  )
  title(main = paste("Positive Words for", uni), line = 2)

  wordcloud(
    words = negative_words$word, freq = negative_words$n,
    scale = c(3, 0.5), max.words = 100, random.order = FALSE,
    rot.per = 0.35, colors = brewer.pal(8, "Reds")
  )
  title(main = paste("Negative Words for", uni), line = 2)
}
```

## Question 4: What specific advice can you give us as communication department of BFH based on your analysis? How can we integrate the analysis of tweets in our internal processes, can you think of any data products that would be of value for us?

### Summary key insights from the analysis
```{r}
# Language Analysis
tweets %>%
  count(lang) %>%
  arrange(desc(n))

# Emoji Analysis
emoji_count <- tweets %>%
  top_n_emojis(full_text)

emoji_count %>%
  mutate(emoji_name = reorder(emoji_name, n)) %>%
  ggplot(aes(n, emoji_name)) +
  geom_col() +
  labs(x = "Count", y = NULL, title = "Top 20 Emojis Used")

insights <- list(
  "Most Active Hours" = hours_with_most_tweets_by_uni,
  "Most Active Days" = days_with_most_tweets_by_uni,
  "Most Common Time Intervals" = most_common_interval_minutes,
  "Content Analysis" = head(word_freqs),
  "Sentiment Analysis" = tweet_sentiment
)
```
# Recommendations:
# 1. Optimize tweet release times based on peak engagement hours.
# 2. Focus on specific days with high activity for important announcements.
# 3. Utilize sentiment analysis to tailor content that resonates positively with the audience.
# 4. Implement topic modeling to identify key themes and align communication strategy accordingly.

